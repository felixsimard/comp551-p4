{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Felix-QAT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0ZCZ6mn6zZEy96fSztSDs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felixsimard/comp551-p4/blob/main/Felix_QAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uL_BAMoHrezM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from resnet import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_random_seeds(random_seed=0):\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "\n",
        "\n",
        "def prepare_dataloader(num_workers=2,\n",
        "                       train_batch_size=128,\n",
        "                       eval_batch_size=256):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                             std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                             std=(0.229, 0.224, 0.225))\n",
        "    ])\n",
        "\n",
        "    train_set = torchvision.datasets.CIFAR10(root=\"data\",\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=train_transform)\n",
        "    # We will use test set for validation and test in this project.\n",
        "    # Do not use test set for validation in practice!\n",
        "    test_set = torchvision.datasets.CIFAR10(root=\"data\",\n",
        "                                            train=False,\n",
        "                                            download=True,\n",
        "                                            transform=test_transform)\n",
        "\n",
        "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
        "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
        "                                               batch_size=train_batch_size,\n",
        "                                               sampler=train_sampler,\n",
        "                                               num_workers=num_workers)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
        "                                              batch_size=eval_batch_size,\n",
        "                                              sampler=test_sampler,\n",
        "                                              num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "n-ABF30krzYl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device, criterion=None):\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    running_loss = 0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels).item()\n",
        "        else:\n",
        "            loss = 0\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    eval_loss = running_loss / len(test_loader.dataset)\n",
        "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
        "\n",
        "    return eval_loss, eval_accuracy"
      ],
      "metadata": {
        "id": "Pg_87TYir2Rc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,\n",
        "                train_loader,\n",
        "                test_loader,\n",
        "                device,\n",
        "                learning_rate=1e-1,\n",
        "                num_epochs=5):\n",
        "\n",
        "    # The training configurations were not carefully selected.\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          momentum=0.9,\n",
        "                          weight_decay=1e-4)\n",
        "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                     milestones=[100, 150],\n",
        "                                                     gamma=0.1,\n",
        "                                                     last_epoch=-1)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = evaluate_model(model=model,\n",
        "                                              test_loader=test_loader,\n",
        "                                              device=device,\n",
        "                                              criterion=criterion)\n",
        "    print(\"Epoch: {:03d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(\n",
        "        0, eval_loss, eval_accuracy))\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        eval_loss, eval_accuracy = evaluate_model(model=model,\n",
        "                                                  test_loader=test_loader,\n",
        "                                                  device=device,\n",
        "                                                  criterion=criterion)\n",
        "\n",
        "        # Set learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\n",
        "            \"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\"\n",
        "            .format(epoch + 1, train_loss, train_accuracy, eval_loss,\n",
        "                    eval_accuracy))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "qaft67GAr6UH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        _ = model(inputs)"
      ],
      "metadata": {
        "id": "oimY6OECr8Xx"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_inference_latency(model,\n",
        "                              device,\n",
        "                              input_size=(1, 3, 32, 32),\n",
        "                              num_samples=100,\n",
        "                              num_warmups=10):\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    x = torch.rand(size=input_size).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_warmups):\n",
        "            _ = model(x)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        for _ in range(num_samples):\n",
        "            _ = model(x)\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_time_ave = elapsed_time / num_samples\n",
        "\n",
        "    return elapsed_time_ave"
      ],
      "metadata": {
        "id": "Qf6ktxGCr-St"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.save(model.state_dict(), model_filepath)\n",
        "\n",
        "\n",
        "def load_model(model, model_filepath, device):\n",
        "\n",
        "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "RMnu_r5Vr_dm"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_torchscript_model(model, model_dir, model_filename):\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    model_filepath = os.path.join(model_dir, model_filename)\n",
        "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
        "    \n",
        "def load_torchscript_model(model_filepath, device):\n",
        "\n",
        "    model = torch.jit.load(model_filepath, map_location=device)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QkKDHbkysA2W"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "\n",
        "    # The number of channels in ResNet18 is divisible by 8.\n",
        "    # This is required for fast GEMM integer matrix multiplication.\n",
        "    # model = torchvision.models.resnet18(pretrained=False)\n",
        "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
        "\n",
        "    # We would use the pretrained ResNet18 as a feature extractor.\n",
        "    # for param in model.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "    # Modify the last FC layer\n",
        "    # num_features = model.fc.in_features\n",
        "    # model.fc = nn.Linear(num_features, 10)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "LImqRogKsFOB"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedResNet18(nn.Module):\n",
        "    def __init__(self, model_fp32):\n",
        "\n",
        "        super(QuantizedResNet18, self).__init__()\n",
        "        # QuantStub converts tensors from floating point to quantized.\n",
        "        # This will only be used for inputs.\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        # DeQuantStub converts tensors from quantized to floating point.\n",
        "        # This will only be used for outputs.\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "        # FP32 model\n",
        "        self.model_fp32 = model_fp32\n",
        "\n",
        "    def forward(self, x):\n",
        "        # manually specify where tensors will be converted from floating\n",
        "        # point to quantized in the quantized model\n",
        "        x = self.quant(x)\n",
        "        x = self.model_fp32(x)\n",
        "        # manually specify where tensors will be converted from quantized\n",
        "        # to floating point in the quantized model\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "T3FhOIJ3sHk5"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_equivalence(model_1,\n",
        "                      model_2,\n",
        "                      device,\n",
        "                      rtol=1e-05,\n",
        "                      atol=1e-08,\n",
        "                      num_tests=100,\n",
        "                      input_size=(1, 3, 32, 32)):\n",
        "\n",
        "    model_1.to(device)\n",
        "    model_2.to(device)\n",
        "\n",
        "    for _ in range(num_tests):\n",
        "        x = torch.rand(size=input_size).to(device)\n",
        "        y1 = model_1(x).detach().cpu().numpy()\n",
        "        y2 = model_2(x).detach().cpu().numpy()\n",
        "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol,\n",
        "                       equal_nan=False) == False:\n",
        "            print(\"Model equivalence test sample failed: \")\n",
        "            print(y1)\n",
        "            print(y2)\n",
        "            return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "sS64U1p8sJGo"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random_seed = 0\n",
        "num_classes = 10\n",
        "cuda_device = torch.device(\"cuda:0\")\n",
        "cpu_device = torch.device(\"cpu:0\")\n",
        "\n",
        "model_dir = \"saved_models\"\n",
        "model_filename = \"resnet18_cifar10.pt\"\n",
        "quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
        "model_filepath = os.path.join(model_dir, model_filename)\n",
        "quantized_model_filepath = os.path.join(model_dir,\n",
        "                                            quantized_model_filename)\n",
        "\n",
        "set_random_seeds(random_seed=random_seed)\n"
      ],
      "metadata": {
        "id": "_vxBoQ_irkzf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an untrained model.\n",
        "model = create_model(num_classes=num_classes)\n",
        "train_loader, test_loader = prepare_dataloader(num_workers=2,\n",
        "                                                   train_batch_size=128,\n",
        "                                                   eval_batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk2qFfEOypGF",
        "outputId": "8cd65258-c320-4dd1-b8d8-8ab07be2ec9f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model.\n",
        "print(\"Training Model...\")\n",
        "model = train_model(model=model,\n",
        "                        train_loader=train_loader,\n",
        "                        test_loader=test_loader,\n",
        "                        device=cuda_device,\n",
        "                        learning_rate=1e-1,\n",
        "                        num_epochs=5)\n",
        "# Save model.\n",
        "save_model(model=model, model_dir=model_dir, model_filename=model_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf_zO7kuyl_F",
        "outputId": "32a8d430-f5c3-4fcc-9189-8fc00fea4011"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n",
            "Epoch: 000 Eval Loss: 2.325 Eval Acc: 0.098\n",
            "Epoch: 001 Train Loss: 2.073 Train Acc: 0.308 Eval Loss: 1.539 Eval Acc: 0.438\n",
            "Epoch: 002 Train Loss: 1.494 Train Acc: 0.452 Eval Loss: 1.337 Eval Acc: 0.510\n",
            "Epoch: 003 Train Loss: 1.315 Train Acc: 0.524 Eval Loss: 1.175 Eval Acc: 0.582\n",
            "Epoch: 004 Train Loss: 1.174 Train Acc: 0.578 Eval Loss: 1.067 Eval Acc: 0.619\n",
            "Epoch: 005 Train Loss: 1.066 Train Acc: 0.621 Eval Loss: 0.994 Eval Acc: 0.649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained model.\n",
        "model = load_model(model=model,\n",
        "                       model_filepath=model_filepath,\n",
        "                       device=cuda_device)\n",
        "# Move the model to CPU since static quantization does not support CUDA currently.\n",
        "model.to(cpu_device)\n",
        "# Make a copy of the model for layer fusion\n",
        "fused_model = copy.deepcopy(model)\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsQJGxvgymPJ",
        "outputId": "5a43d538-b113-466a-a0cf-395187495591"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu1): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (skip_add): FloatFunctional(\n",
              "        (activation_post_process): Identity()\n",
              "      )\n",
              "      (relu2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The model has to be switched to training mode before any layer fusion.\n",
        "    # Otherwise the quantization aware training will not work correctly.\n",
        "fused_model.train()\n",
        "\n",
        "# Fuse the model in place rather manually.\n",
        "fused_model = torch.quantization.fuse_modules(fused_model,\n",
        "                                                  [[\"conv1\", \"bn1\", \"relu\"]],\n",
        "                                                  inplace=True)\n",
        "for module_name, module in fused_model.named_children():\n",
        "    if \"layer\" in module_name:\n",
        "        for basic_block_name, basic_block in module.named_children():\n",
        "            torch.quantization.fuse_modules(\n",
        "                basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]],\n",
        "                inplace=True)\n",
        "            for sub_block_name, sub_block in basic_block.named_children():\n",
        "                if sub_block_name == \"downsample\":\n",
        "                    torch.quantization.fuse_modules(sub_block,\n",
        "                                                        [[\"0\", \"1\"]],\n",
        "                                                        inplace=True)\n",
        "\n",
        "# Print FP32 model.\n",
        "print(model)\n",
        "# Print fused model.\n",
        "print(fused_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmz2v2y20IPD",
        "outputId": "abc26367-506b-4625-c6f7-654d9666d0a5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "ResNet(\n",
            "  (conv1): ConvBnReLU2d(\n",
            "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "  )\n",
            "  (bn1): Identity()\n",
            "  (relu): Identity()\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): ConvBn2d(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): ConvBn2d(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (downsample): Sequential(\n",
            "        (0): ConvBn2d(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (1): Identity()\n",
            "      )\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): ConvBnReLU2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (bn1): Identity()\n",
            "      (relu1): Identity()\n",
            "      (conv2): ConvBn2d(\n",
            "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (bn2): Identity()\n",
            "      (skip_add): FloatFunctional(\n",
            "        (activation_post_process): Identity()\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and fused model should be equivalent.\n",
        "model.eval()\n",
        "fused_model.eval()\n",
        "assert model_equivalence(\n",
        "        model_1=model,\n",
        "        model_2=fused_model,\n",
        "        device=cpu_device,\n",
        "        rtol=1e-03,\n",
        "        atol=1e-06,\n",
        "        num_tests=100,\n",
        "        input_size=(\n",
        "            1, 3, 32,\n",
        "            32)), \"Fused model is not equivalent to the original model!\""
      ],
      "metadata": {
        "id": "2d-g_K9hyiod"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for quantization aware training. This inserts observers in\n",
        "# the model that will observe activation tensors during calibration.\n",
        "quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
        "# Using un-fused model will fail.\n",
        "# Because there is no quantized layer implementation for a single batch normalization layer.\n",
        "# quantized_model = QuantizedResNet18(model_fp32=model)\n",
        "# Select quantization schemes from\n",
        "# https://pytorch.org/docs/stable/quantization-support.html\n",
        "quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
        "# Custom quantization configurations\n",
        "# quantization_config = torch.quantization.default_qconfig\n",
        "# quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
        "\n",
        "quantized_model.qconfig = quantization_config\n",
        "\n",
        "# Print quantization configurations\n",
        "print(quantized_model.qconfig)\n",
        "\n",
        "# https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
        "torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
        "\n",
        "# # Use training data for calibration.\n",
        "print(\"Training QAT Model...\")\n",
        "quantized_model.train()\n",
        "train_model(model=quantized_model,\n",
        "                train_loader=train_loader,\n",
        "                test_loader=test_loader,\n",
        "                device=cuda_device,\n",
        "                learning_rate=1e-3,\n",
        "                num_epochs=5)\n",
        "quantized_model.to(cpu_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYTUIzKCyfMN",
        "outputId": "6248c8b1-55e7-4377-f5f0-e9003573c8c0"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training QAT Model...\n",
            "Epoch: 000 Eval Loss: 0.994 Eval Acc: 0.649\n",
            "Epoch: 001 Train Loss: 0.939 Train Acc: 0.666 Eval Loss: 0.865 Eval Acc: 0.696\n",
            "Epoch: 002 Train Loss: 0.889 Train Acc: 0.685 Eval Loss: 0.855 Eval Acc: 0.699\n",
            "Epoch: 003 Train Loss: 0.875 Train Acc: 0.688 Eval Loss: 0.844 Eval Acc: 0.702\n",
            "Epoch: 004 Train Loss: 0.865 Train Acc: 0.691 Eval Loss: 0.840 Eval Acc: 0.704\n",
            "Epoch: 005 Train Loss: 0.863 Train Acc: 0.694 Eval Loss: 0.834 Eval Acc: 0.706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuantizedResNet18(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): HistogramObserver()\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              "  (model_fp32): ResNet(\n",
              "    (conv1): ConvBnReLU2d(\n",
              "      3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-0.0336, -0.0397, -0.0136, -0.0219, -0.0259, -0.0188, -0.0101, -0.0306,\n",
              "                -0.0015, -0.2028, -0.0012, -0.0199, -0.2052, -0.0292, -0.0048, -0.1059,\n",
              "                -0.0362, -0.0429, -0.5097, -0.1649, -0.0101, -0.0382, -0.0672, -0.1105,\n",
              "                -0.0115, -0.0177, -0.0034, -0.2394, -0.0780, -0.0139, -0.0179, -0.2534,\n",
              "                -0.0405, -0.0260, -0.0272, -0.1262, -0.0174, -0.0326, -0.0376, -0.0021,\n",
              "                -0.4151, -0.0558, -0.0256, -0.0228, -0.0344, -0.0045, -0.0017, -0.0040,\n",
              "                -0.2609, -0.0799, -0.0918, -0.1602, -0.1918, -0.0057, -0.0096, -0.0107,\n",
              "                -0.0214, -0.0045, -0.1510, -0.0017, -0.0127, -0.0145, -0.0488, -0.0182]), max_val=tensor([0.0147, 0.0647, 0.0033, 0.0057, 0.0087, 0.0048, 0.0016, 0.0212, 0.0063,\n",
              "                0.2149, 0.0081, 0.0133, 0.1720, 0.0456, 0.0133, 0.1006, 0.0416, 0.0319,\n",
              "                0.3600, 0.1333, 0.0031, 0.0199, 0.1047, 0.1003, 0.0131, 0.0122, 0.0063,\n",
              "                0.3270, 0.0502, 0.0099, 0.0055, 0.4078, 0.0425, 0.0311, 0.0111, 0.1090,\n",
              "                0.0012, 0.0241, 0.0382, 0.0069, 0.4862, 0.0792, 0.0246, 0.0095, 0.0497,\n",
              "                0.0131, 0.0092, 0.0134, 0.2333, 0.0606, 0.0911, 0.2001, 0.1716, 0.0120,\n",
              "                0.0006, 0.0214, 0.0052, 0.0080, 0.1863, 0.0088, 0.0057, 0.0042, 0.0264,\n",
              "                0.0100])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver()\n",
              "    )\n",
              "    (bn1): Identity()\n",
              "    (relu): Identity()\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0944, -0.0417, -0.0773, -0.0767, -0.0440, -0.1053, -0.0312, -0.0469,\n",
              "                    -0.0373, -0.0409, -0.0522, -0.0318, -0.0549, -0.0660, -0.0600, -0.0676,\n",
              "                    -0.0425, -0.1799, -0.0551, -0.0924, -0.0302, -0.0841, -0.0837, -0.0566,\n",
              "                    -0.0220, -0.1085, -0.0526, -0.1454, -0.0436, -0.0868, -0.0621, -0.0225,\n",
              "                    -0.0544, -0.0573, -0.1192, -0.1039, -0.0543, -0.0777, -0.0384, -0.0703,\n",
              "                    -0.1856, -0.0553, -0.0571, -0.0587, -0.0878, -0.0754, -0.0365, -0.0391,\n",
              "                    -0.0548, -0.0483, -0.0203, -0.0269, -0.0671, -0.0930, -0.0433, -0.0395,\n",
              "                    -0.0896, -0.0272, -0.0376, -0.0344, -0.0573, -0.0945, -0.0727, -0.0529]), max_val=tensor([0.0651, 0.0339, 0.0485, 0.0506, 0.0263, 0.0639, 0.0294, 0.0918, 0.0321,\n",
              "                    0.0406, 0.0446, 0.0341, 0.0482, 0.0552, 0.0701, 0.0563, 0.0473, 0.1324,\n",
              "                    0.0634, 0.0562, 0.0298, 0.0682, 0.0541, 0.0530, 0.0301, 0.1327, 0.0419,\n",
              "                    0.1005, 0.0447, 0.0708, 0.0447, 0.0273, 0.0515, 0.0417, 0.0748, 0.0669,\n",
              "                    0.0473, 0.0529, 0.0526, 0.0566, 0.1161, 0.0411, 0.0553, 0.0707, 0.0704,\n",
              "                    0.0644, 0.0247, 0.0348, 0.0463, 0.0354, 0.0247, 0.0286, 0.0496, 0.0886,\n",
              "                    0.0503, 0.0306, 0.0758, 0.0328, 0.0331, 0.0295, 0.0490, 0.0653, 0.0856,\n",
              "                    0.0449])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.1050, -0.0905, -0.1008, -0.1561, -0.0938, -0.0934, -0.1619, -0.1706,\n",
              "                    -0.1814, -0.0588, -0.1531, -0.1375, -0.0932, -0.1257, -0.1282, -0.0995,\n",
              "                    -0.0988, -0.1896, -0.0435, -0.0826, -0.0747, -0.1475, -0.1917, -0.0455,\n",
              "                    -0.1844, -0.1263, -0.2179, -0.0892, -0.0967, -0.1247, -0.1648, -0.0664,\n",
              "                    -0.1042, -0.1068, -0.1433, -0.0895, -0.1012, -0.1303, -0.1269, -0.0758,\n",
              "                    -0.0756, -0.1127, -0.1514, -0.1782, -0.1039, -0.1882, -0.1761, -0.1125,\n",
              "                    -0.0568, -0.1230, -0.0818, -0.0512, -0.1103, -0.2085, -0.0836, -0.1190,\n",
              "                    -0.1569, -0.1336, -0.0208, -0.1936, -0.1666, -0.0556, -0.1692, -0.1089]), max_val=tensor([0.1286, 0.1046, 0.0982, 0.1382, 0.0796, 0.1057, 0.1745, 0.1927, 0.2215,\n",
              "                    0.0531, 0.1391, 0.1373, 0.1126, 0.1011, 0.1258, 0.0902, 0.1039, 0.1726,\n",
              "                    0.0397, 0.1185, 0.0914, 0.1628, 0.2478, 0.0431, 0.2370, 0.1651, 0.1734,\n",
              "                    0.1090, 0.1415, 0.1020, 0.1324, 0.0715, 0.1238, 0.0825, 0.1354, 0.0953,\n",
              "                    0.0980, 0.1592, 0.1506, 0.1004, 0.0580, 0.0883, 0.2048, 0.1376, 0.1063,\n",
              "                    0.2022, 0.1444, 0.1291, 0.0480, 0.0909, 0.0718, 0.0821, 0.0993, 0.2542,\n",
              "                    0.0976, 0.1140, 0.1465, 0.1941, 0.0244, 0.1559, 0.1268, 0.0917, 0.1397,\n",
              "                    0.1229])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0544, -0.1077, -0.0361, -0.0548, -0.0733, -0.1076, -0.0732, -0.0287,\n",
              "                    -0.0726, -0.0785, -0.0235, -0.1142, -0.0560, -0.0722, -0.0681, -0.0983,\n",
              "                    -0.0712, -0.0718, -0.1030, -0.0585, -0.0440, -0.0525, -0.0188, -0.0610,\n",
              "                    -0.0442, -0.0546, -0.0361, -0.0621, -0.0664, -0.0784, -0.0344, -0.0728,\n",
              "                    -0.0525, -0.0491, -0.0608, -0.0285, -0.0455, -0.0450, -0.0775, -0.1058,\n",
              "                    -0.0529, -0.0811, -0.0493, -0.0779, -0.0818, -0.0521, -0.0670, -0.0467,\n",
              "                    -0.0654, -0.0793, -0.0392, -0.0608, -0.0538, -0.0814, -0.0547, -0.0587,\n",
              "                    -0.0577, -0.0500, -0.0756, -0.0547, -0.0861, -0.0476, -0.0550, -0.0682]), max_val=tensor([0.0505, 0.0700, 0.0545, 0.0763, 0.0604, 0.0835, 0.0934, 0.0437, 0.0856,\n",
              "                    0.0487, 0.0314, 0.0540, 0.0656, 0.1025, 0.0591, 0.0599, 0.0448, 0.0662,\n",
              "                    0.0631, 0.0533, 0.0717, 0.0622, 0.0297, 0.0460, 0.0600, 0.0545, 0.0364,\n",
              "                    0.0474, 0.0856, 0.0980, 0.0352, 0.0600, 0.0465, 0.0436, 0.0511, 0.0426,\n",
              "                    0.0578, 0.0470, 0.0583, 0.0740, 0.0597, 0.0646, 0.0449, 0.0814, 0.0657,\n",
              "                    0.0589, 0.0327, 0.0503, 0.0682, 0.0750, 0.0494, 0.0474, 0.0490, 0.0488,\n",
              "                    0.0609, 0.0425, 0.0748, 0.0537, 0.0543, 0.0476, 0.0681, 0.0597, 0.0558,\n",
              "                    0.0532])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.1285, -0.0922, -0.1378, -0.1190, -0.1481, -0.1222, -0.1350, -0.1188,\n",
              "                    -0.1207, -0.1414, -0.1783, -0.1751, -0.1732, -0.0946, -0.1885, -0.1407,\n",
              "                    -0.1327, -0.1222, -0.1038, -0.0969, -0.1328, -0.1294, -0.1068, -0.0960,\n",
              "                    -0.1597, -0.1915, -0.2311, -0.0920, -0.1614, -0.1621, -0.1266, -0.0835,\n",
              "                    -0.1187, -0.0933, -0.2151, -0.1457, -0.1626, -0.0855, -0.1290, -0.0919,\n",
              "                    -0.0964, -0.1427, -0.1566, -0.1285, -0.1189, -0.1242, -0.2062, -0.1078,\n",
              "                    -0.1228, -0.1595, -0.1192, -0.0817, -0.1033, -0.1518, -0.1184, -0.0977,\n",
              "                    -0.1034, -0.1055, -0.0611, -0.1490, -0.1029, -0.0919, -0.0987, -0.1150]), max_val=tensor([0.1255, 0.1293, 0.1649, 0.1524, 0.1442, 0.1303, 0.1278, 0.1362, 0.1210,\n",
              "                    0.1249, 0.2466, 0.1552, 0.1734, 0.0854, 0.1734, 0.1371, 0.2056, 0.1503,\n",
              "                    0.0932, 0.1152, 0.1560, 0.1302, 0.1645, 0.1018, 0.1938, 0.1991, 0.2008,\n",
              "                    0.1106, 0.1666, 0.1643, 0.1239, 0.0723, 0.1065, 0.1168, 0.1840, 0.1147,\n",
              "                    0.1945, 0.1218, 0.1503, 0.0897, 0.0710, 0.1105, 0.1383, 0.1463, 0.1014,\n",
              "                    0.1179, 0.1817, 0.1189, 0.1060, 0.1231, 0.1193, 0.0720, 0.0847, 0.1278,\n",
              "                    0.1264, 0.1713, 0.1064, 0.0922, 0.0445, 0.1238, 0.0883, 0.1409, 0.1615,\n",
              "                    0.1743])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0521, -0.0765, -0.0477, -0.0719, -0.0490, -0.0416, -0.0738, -0.0474,\n",
              "                    -0.0432, -0.0459, -0.0516, -0.0298, -0.0473, -0.0470, -0.0744, -0.1212,\n",
              "                    -0.0484, -0.0688, -0.0587, -0.0722, -0.0824, -0.0470, -0.0343, -0.0418,\n",
              "                    -0.0498, -0.0461, -0.0518, -0.0687, -0.0517, -0.0360, -0.0429, -0.0371,\n",
              "                    -0.0674, -0.0318, -0.0406, -0.0551, -0.0915, -0.0534, -0.0603, -0.0725,\n",
              "                    -0.0446, -0.0272, -0.0350, -0.0480, -0.0509, -0.0322, -0.0228, -0.0678,\n",
              "                    -0.0377, -0.0370, -0.0665, -0.0415, -0.0703, -0.0503, -0.0334, -0.0662,\n",
              "                    -0.0318, -0.0443, -0.0272, -0.0335, -0.0469, -0.0463, -0.0761, -0.0523,\n",
              "                    -0.0571, -0.0350, -0.0388, -0.0638, -0.0539, -0.0613, -0.0524, -0.0411,\n",
              "                    -0.0491, -0.0366, -0.0446, -0.0334, -0.0385, -0.0282, -0.0385, -0.0991,\n",
              "                    -0.0451, -0.0708, -0.0326, -0.0613, -0.0948, -0.0543, -0.0575, -0.0404,\n",
              "                    -0.0406, -0.0236, -0.0517, -0.0685, -0.0541, -0.0477, -0.0636, -0.0369,\n",
              "                    -0.0426, -0.0478, -0.0461, -0.0562, -0.0448, -0.0622, -0.0455, -0.0513,\n",
              "                    -0.0223, -0.0489, -0.0390, -0.0732, -0.0331, -0.0767, -0.0619, -0.0557,\n",
              "                    -0.0506, -0.0522, -0.0400, -0.0625, -0.0388, -0.0619, -0.0499, -0.0570,\n",
              "                    -0.0758, -0.0775, -0.0741, -0.0248, -0.0491, -0.0559, -0.0368, -0.0441]), max_val=tensor([0.0734, 0.0399, 0.0739, 0.0564, 0.0441, 0.0603, 0.0738, 0.0427, 0.0540,\n",
              "                    0.0538, 0.0734, 0.0350, 0.0488, 0.0405, 0.0746, 0.0702, 0.0483, 0.0706,\n",
              "                    0.0858, 0.0728, 0.0548, 0.0451, 0.0458, 0.0723, 0.0420, 0.0447, 0.0445,\n",
              "                    0.0600, 0.0874, 0.0360, 0.0430, 0.0447, 0.0418, 0.0417, 0.0421, 0.0546,\n",
              "                    0.0610, 0.0602, 0.0897, 0.1067, 0.0434, 0.0371, 0.0452, 0.0612, 0.0425,\n",
              "                    0.0382, 0.0277, 0.0496, 0.0534, 0.0460, 0.0512, 0.0380, 0.0485, 0.0410,\n",
              "                    0.0269, 0.0627, 0.0388, 0.0513, 0.0429, 0.0376, 0.0488, 0.0495, 0.0547,\n",
              "                    0.0549, 0.0376, 0.0347, 0.0611, 0.0419, 0.0566, 0.0805, 0.0481, 0.0511,\n",
              "                    0.0502, 0.0665, 0.0285, 0.0348, 0.0402, 0.0295, 0.0370, 0.0763, 0.0541,\n",
              "                    0.0454, 0.0384, 0.0641, 0.0618, 0.0466, 0.0558, 0.0322, 0.0365, 0.0382,\n",
              "                    0.0329, 0.0405, 0.0761, 0.0474, 0.0469, 0.0389, 0.0608, 0.0735, 0.0451,\n",
              "                    0.0446, 0.0412, 0.0733, 0.0447, 0.0769, 0.0332, 0.0578, 0.0430, 0.0635,\n",
              "                    0.0440, 0.0494, 0.0306, 0.0375, 0.0463, 0.0394, 0.0457, 0.0474, 0.0355,\n",
              "                    0.0614, 0.0530, 0.0541, 0.0484, 0.0438, 0.0644, 0.0397, 0.0647, 0.0465,\n",
              "                    0.0567, 0.0490])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0741, -0.0674, -0.1040, -0.1107, -0.0868, -0.0819, -0.1345, -0.0797,\n",
              "                    -0.0931, -0.0965, -0.0954, -0.0990, -0.0865, -0.0863, -0.0875, -0.0718,\n",
              "                    -0.0872, -0.1082, -0.0565, -0.1149, -0.0662, -0.1047, -0.0858, -0.1479,\n",
              "                    -0.1051, -0.1028, -0.0783, -0.0745, -0.1153, -0.0996, -0.0740, -0.0809,\n",
              "                    -0.0987, -0.0961, -0.1060, -0.0787, -0.0979, -0.0775, -0.1104, -0.1096,\n",
              "                    -0.0959, -0.0481, -0.1118, -0.0795, -0.1190, -0.0856, -0.1072, -0.1151,\n",
              "                    -0.1164, -0.1182, -0.1630, -0.0794, -0.0983, -0.1311, -0.1044, -0.0991,\n",
              "                    -0.0878, -0.0960, -0.1138, -0.0837, -0.0829, -0.0662, -0.0560, -0.1070,\n",
              "                    -0.0570, -0.1436, -0.1108, -0.0822, -0.0876, -0.0972, -0.1028, -0.0839,\n",
              "                    -0.1179, -0.0521, -0.1165, -0.1330, -0.0980, -0.0758, -0.1097, -0.0970,\n",
              "                    -0.1582, -0.0770, -0.0837, -0.1319, -0.0939, -0.1137, -0.1164, -0.0779,\n",
              "                    -0.1478, -0.1376, -0.0693, -0.1354, -0.1365, -0.1041, -0.1101, -0.0894,\n",
              "                    -0.0848, -0.1145, -0.1576, -0.1393, -0.1199, -0.0592, -0.1013, -0.0637,\n",
              "                    -0.0678, -0.1219, -0.0793, -0.1068, -0.0485, -0.1246, -0.1759, -0.1190,\n",
              "                    -0.1447, -0.1021, -0.1336, -0.1375, -0.1221, -0.1033, -0.0733, -0.0662,\n",
              "                    -0.1274, -0.0789, -0.0822, -0.0840, -0.1270, -0.0697, -0.0883, -0.1087]), max_val=tensor([0.0796, 0.0870, 0.0938, 0.1252, 0.0749, 0.0814, 0.1196, 0.1095, 0.0769,\n",
              "                    0.1202, 0.1055, 0.1113, 0.0920, 0.0831, 0.0994, 0.0648, 0.0979, 0.0853,\n",
              "                    0.0710, 0.0863, 0.0855, 0.1021, 0.0820, 0.1656, 0.1298, 0.1838, 0.0946,\n",
              "                    0.1004, 0.1074, 0.1024, 0.0987, 0.0879, 0.1116, 0.1004, 0.1003, 0.0856,\n",
              "                    0.1043, 0.0727, 0.1151, 0.1141, 0.1188, 0.0705, 0.1070, 0.0809, 0.1516,\n",
              "                    0.0871, 0.0960, 0.1269, 0.1326, 0.0916, 0.1497, 0.1284, 0.1089, 0.0922,\n",
              "                    0.1239, 0.0999, 0.1265, 0.0922, 0.1026, 0.1213, 0.0894, 0.0780, 0.0825,\n",
              "                    0.0851, 0.0654, 0.1567, 0.0939, 0.0760, 0.1017, 0.1045, 0.1028, 0.0733,\n",
              "                    0.1201, 0.0712, 0.1045, 0.1507, 0.1123, 0.0990, 0.1265, 0.1112, 0.1421,\n",
              "                    0.1165, 0.0888, 0.1617, 0.0918, 0.1367, 0.1133, 0.0732, 0.1632, 0.1163,\n",
              "                    0.1145, 0.1204, 0.1908, 0.1083, 0.1099, 0.0848, 0.0830, 0.1018, 0.1310,\n",
              "                    0.1520, 0.1206, 0.0719, 0.1296, 0.0834, 0.0961, 0.1905, 0.0988, 0.0941,\n",
              "                    0.0607, 0.1150, 0.1260, 0.0996, 0.1491, 0.0831, 0.1112, 0.1238, 0.1100,\n",
              "                    0.0824, 0.0948, 0.0859, 0.0893, 0.0753, 0.0929, 0.0957, 0.1208, 0.0732,\n",
              "                    0.0787, 0.1184])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (downsample): Sequential(\n",
              "          (0): ConvBn2d(\n",
              "            64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
              "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "              min_val=tensor([-0.3225, -0.1601, -0.2233, -0.2022, -0.3217, -0.1472, -0.1982, -0.2141,\n",
              "                      -0.2146, -0.1314, -0.2082, -0.2142, -0.1867, -0.1268, -0.1899, -0.1760,\n",
              "                      -0.2630, -0.2362, -0.1502, -0.2196, -0.1974, -0.1506, -0.2153, -0.2120,\n",
              "                      -0.1611, -0.2321, -0.1759, -0.2763, -0.2566, -0.2119, -0.2757, -0.1836,\n",
              "                      -0.2478, -0.1749, -0.1756, -0.1950, -0.1441, -0.2127, -0.1333, -0.1676,\n",
              "                      -0.1317, -0.2344, -0.2052, -0.4113, -0.1815, -0.1699, -0.1439, -0.2069,\n",
              "                      -0.1195, -0.2558, -0.2307, -0.1300, -0.1919, -0.2169, -0.1713, -0.1594,\n",
              "                      -0.1892, -0.2311, -0.1220, -0.1866, -0.2170, -0.2234, -0.1659, -0.3842,\n",
              "                      -0.3233, -0.1329, -0.3156, -0.1501, -0.1770, -0.1094, -0.1867, -0.2503,\n",
              "                      -0.1530, -0.2014, -0.1308, -0.2459, -0.2537, -0.1890, -0.2997, -0.3149,\n",
              "                      -0.1522, -0.2121, -0.1585, -0.2589, -0.1707, -0.1553, -0.1945, -0.2200,\n",
              "                      -0.1828, -0.2004, -0.1714, -0.1992, -0.1740, -0.1667, -0.1745, -0.1872,\n",
              "                      -0.1755, -0.3141, -0.1441, -0.2049, -0.1538, -0.4582, -0.2036, -0.1822,\n",
              "                      -0.1164, -0.1808, -0.1831, -0.1833, -0.1719, -0.1505, -0.2198, -0.2045,\n",
              "                      -0.2519, -0.2425, -0.2111, -0.2152, -0.1917, -0.2131, -0.2458, -0.2121,\n",
              "                      -0.1006, -0.1736, -0.2039, -0.2059, -0.1614, -0.2406, -0.1457, -0.2332]), max_val=tensor([0.1805, 0.2009, 0.1634, 0.1721, 0.1209, 0.4396, 0.1518, 0.2110, 0.1544,\n",
              "                      0.1740, 0.1744, 0.2724, 0.1696, 0.2140, 0.1996, 0.2500, 0.1829, 0.2553,\n",
              "                      0.2699, 0.2116, 0.1937, 0.2519, 0.1680, 0.1910, 0.2240, 0.2703, 0.2020,\n",
              "                      0.1576, 0.1637, 0.2490, 0.1695, 0.1976, 0.1751, 0.2356, 0.2318, 0.5704,\n",
              "                      0.1846, 0.1491, 0.1419, 0.1329, 0.2333, 0.1772, 0.1955, 0.2103, 0.3099,\n",
              "                      0.2361, 0.1865, 0.1457, 0.1724, 0.2884, 0.1648, 0.1813, 0.1785, 0.1693,\n",
              "                      0.1794, 0.1897, 0.2282, 0.2150, 0.2042, 0.1122, 0.1781, 0.2962, 0.1826,\n",
              "                      0.1971, 0.2405, 0.1930, 0.2250, 0.2620, 0.1781, 0.1632, 0.2418, 0.2142,\n",
              "                      0.2869, 0.1951, 0.2460, 0.1703, 0.1726, 0.1991, 0.1301, 0.1868, 0.1452,\n",
              "                      0.2061, 0.2069, 0.1809, 0.2606, 0.1467, 0.3015, 0.1832, 0.2231, 0.2304,\n",
              "                      0.1416, 0.1797, 0.1271, 0.2147, 0.2331, 0.2072, 0.1294, 0.1777, 0.1789,\n",
              "                      0.2009, 0.2210, 0.2592, 0.2449, 0.2041, 0.2304, 0.2220, 0.2553, 0.3391,\n",
              "                      0.2050, 0.2075, 0.2123, 0.1888, 0.2126, 0.3871, 0.2387, 0.1944, 0.2789,\n",
              "                      0.1651, 0.2043, 0.1424, 0.1941, 0.3535, 0.2955, 0.3203, 0.1794, 0.2007,\n",
              "                      0.2485, 0.1703])\n",
              "            )\n",
              "            (activation_post_process): HistogramObserver()\n",
              "          )\n",
              "          (1): Identity()\n",
              "        )\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0683, -0.0644, -0.0651, -0.0691, -0.0782, -0.0412, -0.0639, -0.0502,\n",
              "                    -0.0695, -0.0542, -0.0523, -0.0606, -0.0929, -0.0620, -0.0632, -0.0673,\n",
              "                    -0.0494, -0.0731, -0.0716, -0.0660, -0.0923, -0.0695, -0.0716, -0.0693,\n",
              "                    -0.0705, -0.0604, -0.0763, -0.0847, -0.0799, -0.0549, -0.0495, -0.0620,\n",
              "                    -0.0677, -0.0749, -0.0628, -0.0769, -0.0887, -0.0622, -0.0578, -0.0690,\n",
              "                    -0.0773, -0.0652, -0.0561, -0.0485, -0.0712, -0.0640, -0.0783, -0.0634,\n",
              "                    -0.0689, -0.0634, -0.0693, -0.0701, -0.0534, -0.0658, -0.0666, -0.0923,\n",
              "                    -0.0623, -0.1143, -0.0796, -0.0833, -0.0521, -0.0724, -0.0822, -0.1089,\n",
              "                    -0.0494, -0.0728, -0.0649, -0.0725, -0.0657, -0.0519, -0.0721, -0.0671,\n",
              "                    -0.0778, -0.0638, -0.0768, -0.0589, -0.0715, -0.0585, -0.0599, -0.0626,\n",
              "                    -0.0891, -0.0653, -0.0646, -0.0623, -0.0553, -0.0641, -0.0724, -0.0616,\n",
              "                    -0.0818, -0.0811, -0.0514, -0.0781, -0.0601, -0.1061, -0.0819, -0.0551,\n",
              "                    -0.0606, -0.0667, -0.0759, -0.1074, -0.0504, -0.0689, -0.0824, -0.0671,\n",
              "                    -0.0785, -0.0693, -0.0685, -0.0612, -0.0675, -0.0579, -0.0521, -0.0738,\n",
              "                    -0.0648, -0.0559, -0.0553, -0.0835, -0.0702, -0.0706, -0.0388, -0.0691,\n",
              "                    -0.0631, -0.0626, -0.0671, -0.0696, -0.0821, -0.0690, -0.0749, -0.0825]), max_val=tensor([0.0692, 0.0651, 0.0859, 0.0600, 0.0613, 0.0677, 0.0720, 0.0690, 0.0938,\n",
              "                    0.0604, 0.0595, 0.0660, 0.0969, 0.0698, 0.0528, 0.0711, 0.0512, 0.0768,\n",
              "                    0.0752, 0.0706, 0.0690, 0.0559, 0.0776, 0.0633, 0.0819, 0.0665, 0.0545,\n",
              "                    0.1063, 0.0950, 0.0759, 0.0547, 0.0897, 0.0721, 0.0830, 0.0773, 0.0863,\n",
              "                    0.1045, 0.0708, 0.0694, 0.0673, 0.0770, 0.0861, 0.0541, 0.0616, 0.0873,\n",
              "                    0.0665, 0.0711, 0.0731, 0.0671, 0.0574, 0.0669, 0.0819, 0.0756, 0.0734,\n",
              "                    0.0645, 0.0782, 0.0680, 0.0733, 0.0749, 0.0995, 0.0519, 0.0672, 0.0712,\n",
              "                    0.1040, 0.0547, 0.0738, 0.0672, 0.0723, 0.0658, 0.0721, 0.0500, 0.0790,\n",
              "                    0.0859, 0.0674, 0.0800, 0.0638, 0.0554, 0.0645, 0.0798, 0.0850, 0.0938,\n",
              "                    0.0876, 0.0698, 0.0677, 0.0508, 0.0606, 0.0562, 0.0992, 0.0560, 0.0806,\n",
              "                    0.0642, 0.0864, 0.0522, 0.1012, 0.0784, 0.0619, 0.0742, 0.0543, 0.0763,\n",
              "                    0.0965, 0.0560, 0.0743, 0.0894, 0.0665, 0.0896, 0.0602, 0.0661, 0.0624,\n",
              "                    0.0804, 0.0598, 0.0819, 0.0851, 0.0649, 0.0421, 0.0662, 0.0710, 0.0785,\n",
              "                    0.0927, 0.0553, 0.0730, 0.0777, 0.0742, 0.0554, 0.0603, 0.0787, 0.0794,\n",
              "                    0.0697, 0.0783])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0960, -0.0800, -0.0754, -0.1050, -0.0555, -0.0834, -0.0868, -0.0853,\n",
              "                    -0.0890, -0.0829, -0.0697, -0.0898, -0.1202, -0.0979, -0.0614, -0.0708,\n",
              "                    -0.0831, -0.0882, -0.0334, -0.0790, -0.1029, -0.0842, -0.0735, -0.0689,\n",
              "                    -0.0619, -0.0547, -0.0883, -0.0979, -0.0667, -0.0444, -0.0667, -0.0917,\n",
              "                    -0.0623, -0.0524, -0.1045, -0.0806, -0.0594, -0.0695, -0.1085, -0.1136,\n",
              "                    -0.0585, -0.0732, -0.1006, -0.0514, -0.0880, -0.0749, -0.0827, -0.0928,\n",
              "                    -0.0882, -0.0639, -0.0699, -0.0917, -0.0531, -0.0995, -0.0906, -0.0513,\n",
              "                    -0.0729, -0.0681, -0.1742, -0.0769, -0.0821, -0.0868, -0.1134, -0.0757,\n",
              "                    -0.0665, -0.0813, -0.0506, -0.0500, -0.1504, -0.1088, -0.1171, -0.0625,\n",
              "                    -0.0417, -0.0614, -0.0418, -0.0733, -0.1183, -0.0851, -0.0825, -0.0887,\n",
              "                    -0.0910, -0.0733, -0.0880, -0.0876, -0.0738, -0.0952, -0.0697, -0.0691,\n",
              "                    -0.0865, -0.0770, -0.0496, -0.0761, -0.1114, -0.0933, -0.0809, -0.0661,\n",
              "                    -0.0862, -0.0528, -0.0692, -0.0694, -0.0777, -0.0515, -0.0504, -0.0950,\n",
              "                    -0.0827, -0.1087, -0.0900, -0.0808, -0.0803, -0.0892, -0.0710, -0.0615,\n",
              "                    -0.0606, -0.0630, -0.1109, -0.1087, -0.0417, -0.0560, -0.1019, -0.0815,\n",
              "                    -0.0847, -0.0477, -0.0845, -0.0845, -0.0953, -0.0700, -0.0672, -0.0703]), max_val=tensor([0.0756, 0.0845, 0.0914, 0.0927, 0.0524, 0.0919, 0.0678, 0.0584, 0.0865,\n",
              "                    0.0945, 0.0643, 0.0797, 0.1123, 0.0704, 0.0651, 0.0699, 0.1069, 0.0663,\n",
              "                    0.0376, 0.0564, 0.0931, 0.1047, 0.0663, 0.0776, 0.0717, 0.0427, 0.0884,\n",
              "                    0.0834, 0.0738, 0.0543, 0.0622, 0.0872, 0.0626, 0.0464, 0.0711, 0.0702,\n",
              "                    0.0726, 0.0689, 0.0861, 0.1597, 0.0577, 0.0758, 0.0863, 0.0494, 0.1020,\n",
              "                    0.0771, 0.0935, 0.0843, 0.1221, 0.0625, 0.0580, 0.1009, 0.0568, 0.1280,\n",
              "                    0.0952, 0.0625, 0.0675, 0.0776, 0.1131, 0.0732, 0.0844, 0.0786, 0.1063,\n",
              "                    0.0767, 0.0619, 0.0763, 0.0563, 0.0671, 0.1406, 0.1067, 0.0771, 0.0856,\n",
              "                    0.0498, 0.0717, 0.0513, 0.0972, 0.1124, 0.0700, 0.0621, 0.1301, 0.0987,\n",
              "                    0.0602, 0.1024, 0.0782, 0.0730, 0.0807, 0.0749, 0.0632, 0.0965, 0.0798,\n",
              "                    0.0594, 0.0864, 0.1378, 0.1069, 0.0801, 0.0569, 0.0753, 0.0642, 0.0645,\n",
              "                    0.0762, 0.0970, 0.0573, 0.0686, 0.0891, 0.0795, 0.1645, 0.1048, 0.0787,\n",
              "                    0.0961, 0.0859, 0.0739, 0.0595, 0.0500, 0.0814, 0.1274, 0.1076, 0.0470,\n",
              "                    0.0556, 0.0882, 0.1016, 0.0893, 0.0495, 0.0822, 0.1015, 0.0829, 0.0624,\n",
              "                    0.0821, 0.0617])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0473, -0.0539, -0.0719, -0.0666, -0.0672, -0.0500, -0.0587, -0.0498,\n",
              "                    -0.0460, -0.0597, -0.0614, -0.0526, -0.0609, -0.0395, -0.0607, -0.0474,\n",
              "                    -0.0560, -0.0384, -0.0344, -0.0510, -0.0867, -0.0492, -0.0708, -0.0469,\n",
              "                    -0.0575, -0.0539, -0.0519, -0.0504, -0.0481, -0.0625, -0.0544, -0.0478,\n",
              "                    -0.0388, -0.0523, -0.0451, -0.0312, -0.0723, -0.0623, -0.0338, -0.0572,\n",
              "                    -0.0524, -0.0341, -0.0699, -0.0614, -0.0432, -0.0610, -0.0662, -0.0570,\n",
              "                    -0.0462, -0.0726, -0.0636, -0.0519, -0.0525, -0.0475, -0.0578, -0.0387,\n",
              "                    -0.0573, -0.0415, -0.0409, -0.0744, -0.0510, -0.0456, -0.0305, -0.0575,\n",
              "                    -0.0324, -0.0603, -0.0299, -0.0419, -0.0482, -0.0368, -0.0469, -0.0494,\n",
              "                    -0.0495, -0.0275, -0.0343, -0.0346, -0.0659, -0.0444, -0.0461, -0.0517,\n",
              "                    -0.0487, -0.0242, -0.0381, -0.0512, -0.0546, -0.0330, -0.0615, -0.0737,\n",
              "                    -0.0581, -0.0500, -0.0534, -0.0520, -0.0444, -0.0593, -0.0342, -0.0658,\n",
              "                    -0.0271, -0.0650, -0.0596, -0.0428, -0.0654, -0.0316, -0.0536, -0.0646,\n",
              "                    -0.0459, -0.0461, -0.0384, -0.0540, -0.0619, -0.0549, -0.0357, -0.0526,\n",
              "                    -0.0729, -0.0491, -0.0671, -0.0475, -0.0577, -0.0778, -0.0276, -0.0668,\n",
              "                    -0.0687, -0.0360, -0.0402, -0.0504, -0.0843, -0.0632, -0.0549, -0.0501,\n",
              "                    -0.0761, -0.0338, -0.0629, -0.0548, -0.0482, -0.0456, -0.0336, -0.0447,\n",
              "                    -0.0510, -0.0551, -0.0428, -0.0502, -0.0335, -0.0321, -0.0731, -0.0410,\n",
              "                    -0.0318, -0.0464, -0.0812, -0.0531, -0.0518, -0.0275, -0.0383, -0.0478,\n",
              "                    -0.0588, -0.0505, -0.0462, -0.0713, -0.0384, -0.0606, -0.0552, -0.0672,\n",
              "                    -0.0610, -0.0671, -0.0679, -0.0284, -0.0455, -0.0504, -0.0365, -0.0539,\n",
              "                    -0.0366, -0.0546, -0.0475, -0.0292, -0.0640, -0.0494, -0.0456, -0.0526,\n",
              "                    -0.0586, -0.0502, -0.0466, -0.0503, -0.0397, -0.0432, -0.0566, -0.0894,\n",
              "                    -0.0399, -0.0506, -0.0333, -0.0440, -0.0353, -0.0601, -0.0252, -0.0548,\n",
              "                    -0.0629, -0.0401, -0.0426, -0.0619, -0.0583, -0.0421, -0.0486, -0.0670,\n",
              "                    -0.0304, -0.0399, -0.0545, -0.0493, -0.0720, -0.0602, -0.0589, -0.0487,\n",
              "                    -0.0507, -0.0367, -0.0255, -0.0445, -0.0530, -0.0503, -0.0541, -0.0457,\n",
              "                    -0.0256, -0.0404, -0.0694, -0.0422, -0.0678, -0.0480, -0.0354, -0.0464,\n",
              "                    -0.0656, -0.0715, -0.0421, -0.0744, -0.0241, -0.0635, -0.0532, -0.0383,\n",
              "                    -0.0609, -0.0369, -0.0534, -0.0357, -0.0602, -0.0411, -0.0432, -0.0404,\n",
              "                    -0.0413, -0.0520, -0.0341, -0.0516, -0.0421, -0.0691, -0.0451, -0.0486,\n",
              "                    -0.0390, -0.0588, -0.0731, -0.0534, -0.1169, -0.0265, -0.0498, -0.0590]), max_val=tensor([0.0618, 0.0484, 0.0689, 0.0648, 0.0560, 0.0600, 0.0654, 0.0355, 0.0469,\n",
              "                    0.0542, 0.0639, 0.0551, 0.0792, 0.0320, 0.0447, 0.0496, 0.0773, 0.0511,\n",
              "                    0.0460, 0.0497, 0.0879, 0.0550, 0.0589, 0.0577, 0.0725, 0.0396, 0.0409,\n",
              "                    0.0416, 0.0638, 0.0802, 0.0695, 0.0526, 0.0405, 0.0587, 0.0580, 0.0423,\n",
              "                    0.0746, 0.0659, 0.0350, 0.0677, 0.0581, 0.0371, 0.0623, 0.0682, 0.0407,\n",
              "                    0.0445, 0.0607, 0.0601, 0.0509, 0.0812, 0.0848, 0.0670, 0.0743, 0.0395,\n",
              "                    0.0715, 0.0425, 0.0610, 0.0419, 0.0536, 0.0622, 0.0741, 0.0439, 0.0375,\n",
              "                    0.0521, 0.0395, 0.0685, 0.0266, 0.0388, 0.0484, 0.0401, 0.0452, 0.0524,\n",
              "                    0.0584, 0.0331, 0.0570, 0.0333, 0.0705, 0.0472, 0.0424, 0.0636, 0.0397,\n",
              "                    0.0354, 0.0580, 0.0787, 0.0533, 0.0451, 0.0502, 0.0506, 0.0657, 0.0574,\n",
              "                    0.0555, 0.0402, 0.0408, 0.0495, 0.0375, 0.0637, 0.0327, 0.0826, 0.0605,\n",
              "                    0.0375, 0.0743, 0.0289, 0.0818, 0.0618, 0.0519, 0.0489, 0.0409, 0.0406,\n",
              "                    0.0648, 0.0653, 0.0655, 0.0546, 0.0724, 0.0567, 0.0662, 0.0484, 0.0676,\n",
              "                    0.0650, 0.0337, 0.0534, 0.0654, 0.0464, 0.0401, 0.0569, 0.0617, 0.0497,\n",
              "                    0.0724, 0.0647, 0.0935, 0.0357, 0.0598, 0.0622, 0.0521, 0.0598, 0.0278,\n",
              "                    0.0467, 0.0465, 0.0722, 0.0585, 0.0751, 0.0461, 0.0326, 0.0657, 0.0411,\n",
              "                    0.0379, 0.0468, 0.0771, 0.0570, 0.0496, 0.0469, 0.0544, 0.0643, 0.0619,\n",
              "                    0.0619, 0.0492, 0.0514, 0.0404, 0.0776, 0.0618, 0.0826, 0.0491, 0.0752,\n",
              "                    0.0975, 0.0370, 0.0445, 0.0439, 0.0398, 0.0526, 0.0422, 0.0660, 0.0642,\n",
              "                    0.0357, 0.0794, 0.0581, 0.0607, 0.0646, 0.0578, 0.0453, 0.0500, 0.0581,\n",
              "                    0.0451, 0.0417, 0.0550, 0.0610, 0.0490, 0.0501, 0.0393, 0.0409, 0.0523,\n",
              "                    0.0713, 0.0282, 0.0676, 0.0510, 0.0434, 0.0475, 0.0560, 0.0489, 0.0626,\n",
              "                    0.0379, 0.0728, 0.0432, 0.0389, 0.0809, 0.0575, 0.0680, 0.0462, 0.0540,\n",
              "                    0.0692, 0.0592, 0.0467, 0.0303, 0.0414, 0.0524, 0.0524, 0.0616, 0.0505,\n",
              "                    0.0264, 0.0461, 0.0584, 0.0420, 0.0658, 0.0559, 0.0378, 0.0435, 0.0575,\n",
              "                    0.0683, 0.0618, 0.0493, 0.0297, 0.0592, 0.0711, 0.0417, 0.0642, 0.0535,\n",
              "                    0.0489, 0.0398, 0.0552, 0.0341, 0.0291, 0.0413, 0.0515, 0.0450, 0.0375,\n",
              "                    0.0532, 0.0362, 0.0803, 0.0591, 0.0831, 0.0429, 0.0616, 0.0664, 0.0730,\n",
              "                    0.0716, 0.0317, 0.0327, 0.0564])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0520, -0.0718, -0.1196, -0.0584, -0.0589, -0.0695, -0.0628, -0.0483,\n",
              "                    -0.0397, -0.0688, -0.0620, -0.0679, -0.0435, -0.0317, -0.0510, -0.0685,\n",
              "                    -0.0564, -0.0487, -0.0514, -0.1300, -0.0363, -0.0521, -0.0786, -0.0589,\n",
              "                    -0.0597, -0.0433, -0.0492, -0.0594, -0.0403, -0.0584, -0.0559, -0.0940,\n",
              "                    -0.0887, -0.0476, -0.0499, -0.0708, -0.0504, -0.0746, -0.0509, -0.0597,\n",
              "                    -0.0578, -0.0461, -0.0533, -0.0453, -0.0832, -0.0500, -0.0440, -0.0488,\n",
              "                    -0.0699, -0.1205, -0.0574, -0.0490, -0.0615, -0.0334, -0.0441, -0.0567,\n",
              "                    -0.1551, -0.0418, -0.0498, -0.0557, -0.0810, -0.0571, -0.0499, -0.0730,\n",
              "                    -0.1304, -0.0633, -0.0469, -0.0986, -0.0502, -0.0513, -0.0505, -0.1135,\n",
              "                    -0.0902, -0.0415, -0.1322, -0.0592, -0.0375, -0.1267, -0.0603, -0.0360,\n",
              "                    -0.0486, -0.0392, -0.0701, -0.0468, -0.1308, -0.0335, -0.0590, -0.0569,\n",
              "                    -0.0573, -0.0578, -0.0751, -0.0483, -0.0642, -0.0793, -0.0410, -0.0437,\n",
              "                    -0.1042, -0.0726, -0.0539, -0.0480, -0.0370, -0.0820, -0.0498, -0.0625,\n",
              "                    -0.0703, -0.0654, -0.0288, -0.0524, -0.0615, -0.0430, -0.0826, -0.0330,\n",
              "                    -0.0732, -0.0635, -0.0434, -0.0363, -0.0432, -0.1064, -0.0653, -0.1038,\n",
              "                    -0.0449, -0.0304, -0.0457, -0.0620, -0.0666, -0.0488, -0.0409, -0.0453,\n",
              "                    -0.0664, -0.0591, -0.0418, -0.0624, -0.0443, -0.0757, -0.0523, -0.0392,\n",
              "                    -0.0476, -0.0657, -0.0783, -0.0572, -0.1724, -0.0544, -0.0801, -0.0907,\n",
              "                    -0.0527, -0.1190, -0.0485, -0.0609, -0.0520, -0.0643, -0.0337, -0.0344,\n",
              "                    -0.0471, -0.0567, -0.0430, -0.0601, -0.0537, -0.1152, -0.0484, -0.0900,\n",
              "                    -0.0524, -0.0478, -0.0725, -0.0639, -0.0526, -0.0453, -0.1254, -0.0534,\n",
              "                    -0.0850, -0.0518, -0.0394, -0.0597, -0.0680, -0.0858, -0.0828, -0.0479,\n",
              "                    -0.0659, -0.0680, -0.0575, -0.1227, -0.0528, -0.0746, -0.0452, -0.0523,\n",
              "                    -0.0532, -0.0672, -0.0718, -0.0746, -0.0490, -0.0482, -0.0474, -0.0809,\n",
              "                    -0.0326, -0.0756, -0.0426, -0.0383, -0.0413, -0.0667, -0.0717, -0.1108,\n",
              "                    -0.0642, -0.0416, -0.0365, -0.1275, -0.0281, -0.0548, -0.0691, -0.0746,\n",
              "                    -0.0861, -0.0517, -0.1052, -0.0542, -0.0468, -0.1076, -0.0672, -0.0793,\n",
              "                    -0.0992, -0.0378, -0.0556, -0.0604, -0.0992, -0.0586, -0.0339, -0.0552,\n",
              "                    -0.0594, -0.0853, -0.0562, -0.0508, -0.0828, -0.1291, -0.0893, -0.0466,\n",
              "                    -0.0574, -0.0367, -0.0411, -0.0833, -0.0547, -0.1610, -0.0448, -0.0617,\n",
              "                    -0.1186, -0.0588, -0.0789, -0.0403, -0.0819, -0.0651, -0.0845, -0.0543,\n",
              "                    -0.0528, -0.0919, -0.1330, -0.0666, -0.0442, -0.0734, -0.0682, -0.0542]), max_val=tensor([0.0582, 0.0667, 0.1801, 0.0653, 0.0616, 0.0683, 0.0866, 0.0398, 0.0451,\n",
              "                    0.0604, 0.0561, 0.0732, 0.0431, 0.0358, 0.0481, 0.0707, 0.0525, 0.0520,\n",
              "                    0.0533, 0.2300, 0.0489, 0.0453, 0.1152, 0.0712, 0.0547, 0.0579, 0.0562,\n",
              "                    0.0554, 0.0529, 0.0672, 0.0686, 0.0699, 0.1196, 0.0456, 0.0487, 0.0716,\n",
              "                    0.0454, 0.0679, 0.0590, 0.0649, 0.0564, 0.0490, 0.0474, 0.0468, 0.0661,\n",
              "                    0.0425, 0.0534, 0.0386, 0.1152, 0.1118, 0.0648, 0.0402, 0.0620, 0.0382,\n",
              "                    0.0576, 0.0616, 0.1216, 0.0389, 0.0583, 0.0528, 0.0866, 0.0502, 0.0473,\n",
              "                    0.0745, 0.1338, 0.0743, 0.0527, 0.1079, 0.0410, 0.0541, 0.0458, 0.1253,\n",
              "                    0.0926, 0.0514, 0.1691, 0.0593, 0.0545, 0.1338, 0.0726, 0.0483, 0.0568,\n",
              "                    0.0377, 0.0680, 0.0471, 0.1696, 0.0376, 0.0559, 0.0727, 0.0550, 0.0614,\n",
              "                    0.0544, 0.0551, 0.0664, 0.0867, 0.0509, 0.0461, 0.1566, 0.0776, 0.0574,\n",
              "                    0.0491, 0.0509, 0.0837, 0.0427, 0.0599, 0.0905, 0.0582, 0.0353, 0.0667,\n",
              "                    0.0644, 0.0474, 0.1121, 0.0480, 0.0809, 0.0720, 0.0462, 0.0521, 0.0465,\n",
              "                    0.1382, 0.0636, 0.0733, 0.0807, 0.0373, 0.0619, 0.0647, 0.0541, 0.0664,\n",
              "                    0.0399, 0.0448, 0.0708, 0.0656, 0.0577, 0.0774, 0.0422, 0.0957, 0.0743,\n",
              "                    0.0506, 0.0432, 0.0584, 0.0574, 0.0553, 0.1981, 0.0571, 0.0670, 0.1018,\n",
              "                    0.0659, 0.1136, 0.0682, 0.0642, 0.0502, 0.0669, 0.0389, 0.0430, 0.0524,\n",
              "                    0.0812, 0.0469, 0.0637, 0.0643, 0.1645, 0.0487, 0.0760, 0.0473, 0.0530,\n",
              "                    0.1333, 0.0655, 0.0418, 0.0534, 0.1770, 0.0481, 0.0609, 0.0508, 0.0406,\n",
              "                    0.0692, 0.0941, 0.1000, 0.0887, 0.0663, 0.0830, 0.0725, 0.0535, 0.1763,\n",
              "                    0.0666, 0.0681, 0.0553, 0.0496, 0.0561, 0.0765, 0.0538, 0.0882, 0.0539,\n",
              "                    0.0457, 0.0334, 0.1005, 0.0527, 0.0739, 0.0488, 0.0479, 0.0383, 0.0944,\n",
              "                    0.0783, 0.1035, 0.0816, 0.0600, 0.0467, 0.1894, 0.0382, 0.0568, 0.0687,\n",
              "                    0.0845, 0.0954, 0.0461, 0.1134, 0.0557, 0.0477, 0.2010, 0.0677, 0.0628,\n",
              "                    0.0980, 0.0404, 0.0987, 0.0637, 0.0910, 0.0699, 0.0447, 0.0550, 0.0543,\n",
              "                    0.1086, 0.0625, 0.0558, 0.1557, 0.2395, 0.1655, 0.0602, 0.0638, 0.0441,\n",
              "                    0.0404, 0.1056, 0.0636, 0.1562, 0.0411, 0.0451, 0.1299, 0.0664, 0.0676,\n",
              "                    0.0391, 0.0893, 0.0596, 0.0962, 0.0523, 0.0573, 0.0817, 0.1085, 0.0614,\n",
              "                    0.0543, 0.0775, 0.0598, 0.0407])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (downsample): Sequential(\n",
              "          (0): ConvBn2d(\n",
              "            128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
              "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "              min_val=tensor([-0.2067, -0.2093, -0.1220, -0.1254, -0.1754, -0.2067, -0.1676, -0.0945,\n",
              "                      -0.2360, -0.1771, -0.1525, -0.2677, -0.1564, -0.1664, -0.2699, -0.1642,\n",
              "                      -0.2058, -0.0993, -0.2592, -0.1076, -0.2077, -0.1743, -0.1011, -0.1232,\n",
              "                      -0.1774, -0.1852, -0.1556, -0.2536, -0.1969, -0.1804, -0.1912, -0.1641,\n",
              "                      -0.1989, -0.1416, -0.1627, -0.1520, -0.0904, -0.1697, -0.1256, -0.1943,\n",
              "                      -0.1124, -0.2634, -0.1776, -0.2274, -0.1551, -0.1320, -0.1928, -0.2584,\n",
              "                      -0.1297, -0.1592, -0.1569, -0.1219, -0.1565, -0.1751, -0.1755, -0.1788,\n",
              "                      -0.1779, -0.1577, -0.1663, -0.1627, -0.1335, -0.1285, -0.1666, -0.2052,\n",
              "                      -0.1042, -0.1463, -0.1919, -0.0630, -0.1516, -0.1617, -0.1556, -0.0580,\n",
              "                      -0.1485, -0.1737, -0.1219, -0.1242, -0.2565, -0.1775, -0.1124, -0.1336,\n",
              "                      -0.1976, -0.1508, -0.1877, -0.2167, -0.1320, -0.1994, -0.1533, -0.0882,\n",
              "                      -0.2068, -0.1901, -0.1584, -0.2010, -0.2128, -0.1905, -0.1729, -0.2163,\n",
              "                      -0.1989, -0.1861, -0.1478, -0.1446, -0.1151, -0.1750, -0.1540, -0.1755,\n",
              "                      -0.1792, -0.1550, -0.1471, -0.1676, -0.1620, -0.1989, -0.0549, -0.1568,\n",
              "                      -0.1872, -0.1623, -0.1086, -0.1476, -0.1560, -0.0985, -0.2713, -0.1873,\n",
              "                      -0.2505, -0.1174, -0.1681, -0.1434, -0.1818, -0.1011, -0.1873, -0.1781,\n",
              "                      -0.1809, -0.2022, -0.1891, -0.1469, -0.1974, -0.1505, -0.1562, -0.2124,\n",
              "                      -0.2056, -0.1779, -0.1204, -0.1087, -0.0923, -0.1604, -0.1322, -0.1683,\n",
              "                      -0.2178, -0.0712, -0.0806, -0.1257, -0.2139, -0.1575, -0.0999, -0.0892,\n",
              "                      -0.1572, -0.1345, -0.1390, -0.1782, -0.1938, -0.1995, -0.1941, -0.1081,\n",
              "                      -0.1791, -0.1829, -0.1785, -0.1292, -0.1593, -0.1413, -0.1047, -0.2415,\n",
              "                      -0.2552, -0.2054, -0.1716, -0.2517, -0.1812, -0.2217, -0.1108, -0.1326,\n",
              "                      -0.1804, -0.2126, -0.1742, -0.0587, -0.1437, -0.1650, -0.1860, -0.2064,\n",
              "                      -0.1590, -0.2032, -0.1565, -0.1153, -0.1320, -0.1833, -0.1787, -0.2240,\n",
              "                      -0.1061, -0.1545, -0.1581, -0.1597, -0.2169, -0.1946, -0.1682, -0.1094,\n",
              "                      -0.1771, -0.0878, -0.1706, -0.1638, -0.1475, -0.2051, -0.1294, -0.0841,\n",
              "                      -0.2306, -0.2103, -0.1045, -0.2173, -0.2234, -0.1227, -0.1596, -0.2272,\n",
              "                      -0.2544, -0.1538, -0.1147, -0.1628, -0.1872, -0.1236, -0.1222, -0.2214,\n",
              "                      -0.2665, -0.0941, -0.2111, -0.1871, -0.0737, -0.0981, -0.0965, -0.1974,\n",
              "                      -0.1825, -0.1391, -0.1575, -0.1881, -0.1081, -0.0683, -0.2328, -0.1084,\n",
              "                      -0.1080, -0.2026, -0.1107, -0.1336, -0.2279, -0.2305, -0.1063, -0.1805,\n",
              "                      -0.2824, -0.1636, -0.1758, -0.1639, -0.1200, -0.1427, -0.1740, -0.1712]), max_val=tensor([0.2229, 0.2817, 0.1276, 0.1088, 0.2331, 0.2836, 0.1692, 0.1013, 0.2217,\n",
              "                      0.2778, 0.1688, 0.2303, 0.1467, 0.1261, 0.2459, 0.1930, 0.2016, 0.1400,\n",
              "                      0.1694, 0.1024, 0.2289, 0.1481, 0.1033, 0.1268, 0.1853, 0.1744, 0.1480,\n",
              "                      0.1665, 0.1677, 0.1996, 0.1727, 0.1944, 0.1699, 0.1868, 0.1916, 0.1763,\n",
              "                      0.1059, 0.2241, 0.1540, 0.2436, 0.1235, 0.2773, 0.1998, 0.1572, 0.1967,\n",
              "                      0.1440, 0.1419, 0.3082, 0.1505, 0.1615, 0.1532, 0.1316, 0.1631, 0.2329,\n",
              "                      0.1402, 0.1615, 0.1146, 0.1822, 0.1614, 0.2015, 0.1660, 0.1756, 0.1524,\n",
              "                      0.1542, 0.1023, 0.2479, 0.1596, 0.0642, 0.2119, 0.1674, 0.1369, 0.0950,\n",
              "                      0.1525, 0.2188, 0.1232, 0.1619, 0.2252, 0.1861, 0.1600, 0.1182, 0.2205,\n",
              "                      0.1539, 0.1962, 0.1831, 0.1022, 0.2764, 0.1892, 0.1129, 0.1724, 0.1733,\n",
              "                      0.1248, 0.2017, 0.2437, 0.1500, 0.1632, 0.2661, 0.1886, 0.1772, 0.1498,\n",
              "                      0.1905, 0.1405, 0.1414, 0.1677, 0.1924, 0.2547, 0.1296, 0.1980, 0.1882,\n",
              "                      0.1670, 0.1495, 0.0836, 0.1384, 0.1481, 0.1706, 0.1724, 0.1759, 0.2274,\n",
              "                      0.1018, 0.1957, 0.1770, 0.2192, 0.1340, 0.1592, 0.1738, 0.1784, 0.0811,\n",
              "                      0.2001, 0.1406, 0.1798, 0.2136, 0.1655, 0.1248, 0.1772, 0.1606, 0.2111,\n",
              "                      0.2356, 0.1799, 0.2449, 0.1493, 0.1710, 0.1180, 0.2475, 0.1574, 0.1080,\n",
              "                      0.2694, 0.0761, 0.1463, 0.0891, 0.3142, 0.1500, 0.1211, 0.1001, 0.1469,\n",
              "                      0.1592, 0.1534, 0.1942, 0.2300, 0.1290, 0.2014, 0.1655, 0.2488, 0.1836,\n",
              "                      0.1705, 0.1336, 0.1518, 0.1191, 0.1306, 0.1679, 0.1078, 0.2415, 0.2101,\n",
              "                      0.2246, 0.1937, 0.1684, 0.1155, 0.1027, 0.1373, 0.2489, 0.1684, 0.0504,\n",
              "                      0.1788, 0.1818, 0.1404, 0.1052, 0.2114, 0.1487, 0.1715, 0.1347, 0.1631,\n",
              "                      0.2064, 0.1472, 0.2052, 0.1088, 0.1576, 0.1631, 0.1563, 0.1690, 0.1766,\n",
              "                      0.2010, 0.0914, 0.1859, 0.1257, 0.2079, 0.1371, 0.1392, 0.1690, 0.0635,\n",
              "                      0.1109, 0.1956, 0.1603, 0.1043, 0.1381, 0.1735, 0.1179, 0.1742, 0.2270,\n",
              "                      0.1947, 0.2113, 0.1625, 0.1712, 0.1718, 0.1061, 0.1263, 0.2001, 0.2954,\n",
              "                      0.0985, 0.1820, 0.1838, 0.0983, 0.0762, 0.0966, 0.2076, 0.1840, 0.1076,\n",
              "                      0.1951, 0.3393, 0.1037, 0.0670, 0.2547, 0.1230, 0.0786, 0.1576, 0.1890,\n",
              "                      0.1442, 0.2580, 0.3323, 0.1064, 0.1847, 0.2281, 0.1807, 0.1678, 0.2161,\n",
              "                      0.1607, 0.0776, 0.1769, 0.1395])\n",
              "            )\n",
              "            (activation_post_process): HistogramObserver()\n",
              "          )\n",
              "          (1): Identity()\n",
              "        )\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0350, -0.0752, -0.0338, -0.0279, -0.0476, -0.0517, -0.0419, -0.0451,\n",
              "                    -0.0432, -0.0433, -0.0401, -0.0519, -0.0464, -0.0308, -0.0477, -0.0326,\n",
              "                    -0.0346, -0.0358, -0.0375, -0.0537, -0.0380, -0.0402, -0.0318, -0.0394,\n",
              "                    -0.0299, -0.0348, -0.0434, -0.0560, -0.0396, -0.0302, -0.0621, -0.0327,\n",
              "                    -0.0372, -0.0342, -0.0387, -0.0304, -0.0455, -0.0353, -0.0347, -0.0480,\n",
              "                    -0.0293, -0.0364, -0.0477, -0.0470, -0.0277, -0.0612, -0.0521, -0.0286,\n",
              "                    -0.0402, -0.0406, -0.0449, -0.0284, -0.0315, -0.0399, -0.0421, -0.0338,\n",
              "                    -0.0325, -0.0493, -0.0386, -0.0671, -0.0401, -0.0252, -0.0428, -0.0507,\n",
              "                    -0.0270, -0.0381, -0.0440, -0.0391, -0.0455, -0.0421, -0.0197, -0.0367,\n",
              "                    -0.0337, -0.0420, -0.0423, -0.0349, -0.0535, -0.0350, -0.0370, -0.0361,\n",
              "                    -0.0372, -0.0334, -0.0304, -0.0389, -0.0354, -0.0371, -0.0443, -0.0304,\n",
              "                    -0.0418, -0.0416, -0.0477, -0.0428, -0.0411, -0.0437, -0.0436, -0.0300,\n",
              "                    -0.0370, -0.0431, -0.0338, -0.0336, -0.0414, -0.0338, -0.0421, -0.0315,\n",
              "                    -0.0621, -0.0432, -0.0334, -0.0498, -0.0295, -0.0315, -0.0369, -0.0795,\n",
              "                    -0.0353, -0.0418, -0.0446, -0.0404, -0.0427, -0.0623, -0.0616, -0.0409,\n",
              "                    -0.0333, -0.0338, -0.0354, -0.0294, -0.0246, -0.0340, -0.0415, -0.0464,\n",
              "                    -0.0311, -0.0323, -0.0484, -0.0302, -0.0397, -0.0471, -0.0468, -0.0497,\n",
              "                    -0.0269, -0.0245, -0.0429, -0.0480, -0.0335, -0.0287, -0.0431, -0.0468,\n",
              "                    -0.0422, -0.0351, -0.0695, -0.0421, -0.0365, -0.0375, -0.0408, -0.0503,\n",
              "                    -0.0415, -0.0479, -0.0441, -0.0395, -0.0277, -0.0259, -0.0401, -0.0426,\n",
              "                    -0.0464, -0.0465, -0.0362, -0.0439, -0.0550, -0.0604, -0.0504, -0.0384,\n",
              "                    -0.0483, -0.0262, -0.0267, -0.0450, -0.0352, -0.0280, -0.0415, -0.0404,\n",
              "                    -0.0631, -0.0387, -0.0372, -0.0300, -0.0625, -0.0451, -0.0451, -0.0405,\n",
              "                    -0.0458, -0.0281, -0.0476, -0.0582, -0.0391, -0.0392, -0.0245, -0.0390,\n",
              "                    -0.0343, -0.0403, -0.0326, -0.0569, -0.0279, -0.0335, -0.0285, -0.0467,\n",
              "                    -0.0333, -0.0252, -0.0286, -0.0654, -0.0394, -0.0357, -0.0346, -0.0330,\n",
              "                    -0.0310, -0.0347, -0.0382, -0.0597, -0.0484, -0.0305, -0.0318, -0.0393,\n",
              "                    -0.0486, -0.0359, -0.0568, -0.0205, -0.0357, -0.0290, -0.0278, -0.0477,\n",
              "                    -0.0374, -0.0484, -0.0378, -0.0354, -0.0342, -0.0496, -0.0289, -0.0430,\n",
              "                    -0.0380, -0.0319, -0.0308, -0.0372, -0.0215, -0.0364, -0.0353, -0.0493,\n",
              "                    -0.0400, -0.0329, -0.0593, -0.0339, -0.0503, -0.0335, -0.0430, -0.0374,\n",
              "                    -0.0573, -0.0296, -0.0332, -0.0576, -0.0294, -0.0415, -0.0292, -0.0465]), max_val=tensor([0.0311, 0.0679, 0.0421, 0.0320, 0.0475, 0.0528, 0.0408, 0.0455, 0.0391,\n",
              "                    0.0449, 0.0390, 0.0505, 0.0441, 0.0269, 0.0540, 0.0286, 0.0392, 0.0423,\n",
              "                    0.0363, 0.0434, 0.0330, 0.0427, 0.0350, 0.0408, 0.0295, 0.0297, 0.0404,\n",
              "                    0.0491, 0.0441, 0.0348, 0.0545, 0.0408, 0.0314, 0.0290, 0.0394, 0.0398,\n",
              "                    0.0464, 0.0378, 0.0388, 0.0468, 0.0310, 0.0337, 0.0488, 0.0493, 0.0302,\n",
              "                    0.0699, 0.0533, 0.0291, 0.0631, 0.0391, 0.0600, 0.0339, 0.0315, 0.0356,\n",
              "                    0.0412, 0.0285, 0.0348, 0.0510, 0.0507, 0.0467, 0.0407, 0.0251, 0.0441,\n",
              "                    0.0523, 0.0300, 0.0457, 0.0542, 0.0504, 0.0645, 0.0558, 0.0241, 0.0507,\n",
              "                    0.0329, 0.0432, 0.0453, 0.0387, 0.0449, 0.0373, 0.0355, 0.0365, 0.0431,\n",
              "                    0.0318, 0.0317, 0.0310, 0.0440, 0.0381, 0.0542, 0.0308, 0.0461, 0.0527,\n",
              "                    0.0533, 0.0508, 0.0330, 0.0520, 0.0444, 0.0277, 0.0459, 0.0319, 0.0345,\n",
              "                    0.0363, 0.0452, 0.0316, 0.0481, 0.0388, 0.0699, 0.0455, 0.0373, 0.0545,\n",
              "                    0.0353, 0.0389, 0.0403, 0.0693, 0.0313, 0.0443, 0.0469, 0.0450, 0.0381,\n",
              "                    0.0571, 0.0701, 0.0411, 0.0373, 0.0398, 0.0304, 0.0377, 0.0229, 0.0368,\n",
              "                    0.0455, 0.0422, 0.0429, 0.0357, 0.0572, 0.0372, 0.0586, 0.0409, 0.0497,\n",
              "                    0.0487, 0.0280, 0.0250, 0.0371, 0.0530, 0.0434, 0.0283, 0.0408, 0.0546,\n",
              "                    0.0566, 0.0349, 0.0732, 0.0433, 0.0379, 0.0410, 0.0452, 0.0544, 0.0351,\n",
              "                    0.0540, 0.0483, 0.0442, 0.0340, 0.0322, 0.0409, 0.0584, 0.0504, 0.0356,\n",
              "                    0.0340, 0.0553, 0.0630, 0.0860, 0.0436, 0.0452, 0.0414, 0.0263, 0.0320,\n",
              "                    0.0403, 0.0484, 0.0407, 0.0512, 0.0419, 0.0480, 0.0620, 0.0424, 0.0344,\n",
              "                    0.0634, 0.0463, 0.0540, 0.0409, 0.0429, 0.0301, 0.0414, 0.0674, 0.0380,\n",
              "                    0.0375, 0.0300, 0.0432, 0.0341, 0.0395, 0.0424, 0.0543, 0.0264, 0.0359,\n",
              "                    0.0380, 0.0465, 0.0390, 0.0390, 0.0318, 0.0627, 0.0444, 0.0392, 0.0376,\n",
              "                    0.0405, 0.0323, 0.0340, 0.0391, 0.0742, 0.0503, 0.0317, 0.0289, 0.0489,\n",
              "                    0.0615, 0.0417, 0.0456, 0.0235, 0.0388, 0.0389, 0.0282, 0.0579, 0.0384,\n",
              "                    0.0502, 0.0374, 0.0321, 0.0418, 0.0568, 0.0262, 0.0333, 0.0472, 0.0347,\n",
              "                    0.0299, 0.0404, 0.0272, 0.0561, 0.0423, 0.0424, 0.0414, 0.0323, 0.0575,\n",
              "                    0.0250, 0.0530, 0.0385, 0.0505, 0.0370, 0.0503, 0.0329, 0.0338, 0.0483,\n",
              "                    0.0324, 0.0432, 0.0277, 0.0899])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0420, -0.0359, -0.0475, -0.0475, -0.0226, -0.0551, -0.0389, -0.0276,\n",
              "                    -0.0236, -0.0414, -0.0423, -0.0513, -0.0399, -0.0177, -0.0366, -0.0267,\n",
              "                    -0.0418, -0.0445, -0.0518, -0.0347, -0.0552, -0.0504, -0.0270, -0.0393,\n",
              "                    -0.0318, -0.0583, -0.0358, -0.0352, -0.0418, -0.0553, -0.0401, -0.0296,\n",
              "                    -0.0598, -0.0451, -0.0384, -0.0358, -0.0341, -0.0255, -0.0505, -0.0412,\n",
              "                    -0.0324, -0.0487, -0.0460, -0.0788, -0.0419, -0.0324, -0.0331, -0.0327,\n",
              "                    -0.0463, -0.0601, -0.0398, -0.0310, -0.0520, -0.0367, -0.0576, -0.0415,\n",
              "                    -0.0251, -0.0545, -0.0424, -0.0432, -0.0397, -0.0350, -0.0439, -0.0424,\n",
              "                    -0.0326, -0.0366, -0.0345, -0.0295, -0.0395, -0.0276, -0.0298, -0.0517,\n",
              "                    -0.0313, -0.0429, -0.0343, -0.0379, -0.0359, -0.0293, -0.0736, -0.0306,\n",
              "                    -0.0342, -0.0212, -0.0413, -0.0426, -0.0278, -0.0345, -0.0394, -0.0284,\n",
              "                    -0.0343, -0.0408, -0.0471, -0.0314, -0.0477, -0.0325, -0.0527, -0.0551,\n",
              "                    -0.0427, -0.0351, -0.0417, -0.0302, -0.0299, -0.0626, -0.0290, -0.0320,\n",
              "                    -0.0524, -0.0727, -0.0299, -0.0491, -0.0444, -0.0411, -0.0223, -0.0257,\n",
              "                    -0.0529, -0.0410, -0.0241, -0.0689, -0.0288, -0.0441, -0.0438, -0.0348,\n",
              "                    -0.0489, -0.0487, -0.0418, -0.0469, -0.0519, -0.0387, -0.0415, -0.0448,\n",
              "                    -0.0300, -0.0466, -0.0346, -0.0280, -0.0414, -0.0548, -0.0362, -0.0478,\n",
              "                    -0.0318, -0.0430, -0.0334, -0.0375, -0.0352, -0.0389, -0.0431, -0.0471,\n",
              "                    -0.0341, -0.0290, -0.0369, -0.0298, -0.0301, -0.0668, -0.0347, -0.0248,\n",
              "                    -0.0365, -0.0399, -0.0273, -0.0511, -0.0508, -0.0424, -0.0548, -0.0302,\n",
              "                    -0.0597, -0.0372, -0.0419, -0.0373, -0.0395, -0.0490, -0.0281, -0.0369,\n",
              "                    -0.0331, -0.0502, -0.0450, -0.0372, -0.0377, -0.0593, -0.0195, -0.0285,\n",
              "                    -0.0375, -0.0468, -0.0385, -0.0222, -0.0503, -0.0386, -0.0401, -0.0278,\n",
              "                    -0.0377, -0.0427, -0.0495, -0.0322, -0.0378, -0.0475, -0.0307, -0.0629,\n",
              "                    -0.0378, -0.0550, -0.0321, -0.0355, -0.0381, -0.0502, -0.0576, -0.0285,\n",
              "                    -0.0356, -0.0339, -0.0411, -0.0447, -0.0587, -0.0298, -0.0430, -0.0426,\n",
              "                    -0.0330, -0.0560, -0.0310, -0.0482, -0.0322, -0.0306, -0.0555, -0.0470,\n",
              "                    -0.0673, -0.0329, -0.0370, -0.0362, -0.0485, -0.0444, -0.0256, -0.0261,\n",
              "                    -0.0349, -0.0368, -0.0542, -0.0505, -0.0276, -0.0271, -0.0257, -0.0309,\n",
              "                    -0.0382, -0.0377, -0.0478, -0.0413, -0.0308, -0.0296, -0.0246, -0.0328,\n",
              "                    -0.0352, -0.0312, -0.0440, -0.0297, -0.0591, -0.0819, -0.0509, -0.0447,\n",
              "                    -0.0486, -0.0238, -0.0457, -0.0288, -0.0526, -0.0478, -0.0347, -0.0471]), max_val=tensor([0.0426, 0.0396, 0.0423, 0.0389, 0.0294, 0.0646, 0.0391, 0.0287, 0.0302,\n",
              "                    0.0395, 0.0447, 0.0423, 0.0510, 0.0266, 0.0392, 0.0350, 0.0423, 0.0434,\n",
              "                    0.0384, 0.0301, 0.0503, 0.0464, 0.0217, 0.0412, 0.0374, 0.0666, 0.0367,\n",
              "                    0.0367, 0.0448, 0.0623, 0.0396, 0.0348, 0.0626, 0.0407, 0.0323, 0.0383,\n",
              "                    0.0346, 0.0259, 0.0477, 0.0439, 0.0312, 0.0527, 0.0478, 0.0642, 0.0364,\n",
              "                    0.0368, 0.0375, 0.0413, 0.0538, 0.0652, 0.0355, 0.0337, 0.0843, 0.0345,\n",
              "                    0.0589, 0.0361, 0.0275, 0.0667, 0.0367, 0.0492, 0.0382, 0.0277, 0.0452,\n",
              "                    0.0357, 0.0230, 0.0338, 0.0429, 0.0223, 0.0384, 0.0260, 0.0339, 0.0570,\n",
              "                    0.0412, 0.0523, 0.0306, 0.0496, 0.0391, 0.0379, 0.0744, 0.0376, 0.0408,\n",
              "                    0.0290, 0.0370, 0.0482, 0.0226, 0.0355, 0.0401, 0.0308, 0.0401, 0.0449,\n",
              "                    0.0693, 0.0429, 0.0555, 0.0382, 0.0498, 0.0445, 0.0410, 0.0362, 0.0365,\n",
              "                    0.0424, 0.0344, 0.0549, 0.0257, 0.0389, 0.0567, 0.0649, 0.0332, 0.0623,\n",
              "                    0.0482, 0.0313, 0.0304, 0.0328, 0.0411, 0.0356, 0.0246, 0.0651, 0.0334,\n",
              "                    0.0475, 0.0581, 0.0481, 0.0507, 0.0390, 0.0463, 0.0479, 0.0439, 0.0319,\n",
              "                    0.0371, 0.0518, 0.0321, 0.0455, 0.0356, 0.0303, 0.0464, 0.0673, 0.0348,\n",
              "                    0.0488, 0.0351, 0.0431, 0.0385, 0.0387, 0.0379, 0.0441, 0.0473, 0.0382,\n",
              "                    0.0290, 0.0220, 0.0396, 0.0323, 0.0277, 0.0546, 0.0386, 0.0311, 0.0328,\n",
              "                    0.0371, 0.0349, 0.0503, 0.0456, 0.0754, 0.0561, 0.0424, 0.0536, 0.0334,\n",
              "                    0.0415, 0.0303, 0.0363, 0.0352, 0.0338, 0.0376, 0.0393, 0.0439, 0.0370,\n",
              "                    0.0317, 0.0328, 0.0661, 0.0197, 0.0416, 0.0389, 0.0409, 0.0339, 0.0241,\n",
              "                    0.0659, 0.0333, 0.0489, 0.0449, 0.0392, 0.0434, 0.0543, 0.0317, 0.0318,\n",
              "                    0.0496, 0.0367, 0.0746, 0.0372, 0.0547, 0.0274, 0.0388, 0.0377, 0.0464,\n",
              "                    0.0553, 0.0346, 0.0382, 0.0386, 0.0353, 0.0549, 0.0644, 0.0312, 0.0434,\n",
              "                    0.0450, 0.0314, 0.0735, 0.0296, 0.0493, 0.0452, 0.0403, 0.0531, 0.0372,\n",
              "                    0.0618, 0.0488, 0.0403, 0.0462, 0.0492, 0.0642, 0.0302, 0.0310, 0.0428,\n",
              "                    0.0313, 0.0438, 0.0459, 0.0270, 0.0256, 0.0379, 0.0450, 0.0384, 0.0363,\n",
              "                    0.0410, 0.0446, 0.0317, 0.0404, 0.0271, 0.0303, 0.0375, 0.0340, 0.0433,\n",
              "                    0.0308, 0.0718, 0.0753, 0.0619, 0.0419, 0.0386, 0.0363, 0.0458, 0.0276,\n",
              "                    0.0499, 0.0497, 0.0344, 0.0427])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0282, -0.0187, -0.0182, -0.0265, -0.0448, -0.0088, -0.0221, -0.0181,\n",
              "                    -0.0293, -0.0188, -0.0423, -0.0256, -0.0318, -0.0252, -0.0311, -0.0123,\n",
              "                    -0.0118, -0.0186, -0.0163, -0.0334, -0.0086, -0.0291, -0.0080, -0.0169,\n",
              "                    -0.0177, -0.0390, -0.0188, -0.0145, -0.0187, -0.0365, -0.0337, -0.0169,\n",
              "                    -0.0140, -0.0090, -0.0128, -0.0308, -0.0182, -0.0223, -0.0250, -0.0127,\n",
              "                    -0.0193, -0.0179, -0.0270, -0.0214, -0.0196, -0.0253, -0.0253, -0.0189,\n",
              "                    -0.0165, -0.0420, -0.0135, -0.0152, -0.0313, -0.0223, -0.0144, -0.0142,\n",
              "                    -0.0139, -0.0229, -0.0223, -0.0242, -0.0214, -0.0244, -0.0169, -0.0324,\n",
              "                    -0.0174, -0.0377, -0.0140, -0.0170, -0.0272, -0.0089, -0.0181, -0.0247,\n",
              "                    -0.0198, -0.0181, -0.0193, -0.0172, -0.0326, -0.0179, -0.0216, -0.0123,\n",
              "                    -0.0100, -0.0179, -0.0175, -0.0142, -0.0227, -0.0434, -0.0124, -0.0242,\n",
              "                    -0.0185, -0.0184, -0.0193, -0.0187, -0.0275, -0.0115, -0.0158, -0.0103,\n",
              "                    -0.0218, -0.0175, -0.0109, -0.0120, -0.0182, -0.0121, -0.0189, -0.0198,\n",
              "                    -0.0192, -0.0253, -0.0221, -0.0099, -0.0213, -0.0160, -0.0151, -0.0121,\n",
              "                    -0.0194, -0.0100, -0.0111, -0.0189, -0.0239, -0.0151, -0.0195, -0.0093,\n",
              "                    -0.0249, -0.0247, -0.0196, -0.0156, -0.0205, -0.0128, -0.0169, -0.0246,\n",
              "                    -0.0215, -0.0149, -0.0158, -0.0554, -0.0182, -0.0170, -0.0155, -0.0147,\n",
              "                    -0.0228, -0.0225, -0.0246, -0.0293, -0.0096, -0.0105, -0.0119, -0.0242,\n",
              "                    -0.0145, -0.0141, -0.0179, -0.0143, -0.0198, -0.0154, -0.0306, -0.0202,\n",
              "                    -0.0383, -0.0214, -0.0133, -0.0179, -0.0182, -0.0176, -0.0137, -0.0207,\n",
              "                    -0.0209, -0.0248, -0.0179, -0.0242, -0.0222, -0.0528, -0.0184, -0.0199,\n",
              "                    -0.0213, -0.0317, -0.0149, -0.0379, -0.0161, -0.0208, -0.0153, -0.0220,\n",
              "                    -0.0181, -0.0117, -0.0452, -0.0239, -0.0200, -0.0255, -0.0289, -0.0221,\n",
              "                    -0.0158, -0.0208, -0.0313, -0.0261, -0.0563, -0.0298, -0.0088, -0.0316,\n",
              "                    -0.0067, -0.0118, -0.0093, -0.0080, -0.0091, -0.0090, -0.0265, -0.0245,\n",
              "                    -0.0228, -0.0104, -0.0120, -0.0168, -0.0239, -0.0075, -0.0144, -0.0163,\n",
              "                    -0.0159, -0.0206, -0.0131, -0.0390, -0.0105, -0.0171, -0.0160, -0.0188,\n",
              "                    -0.0076, -0.0199, -0.0217, -0.0186, -0.0230, -0.0414, -0.0373, -0.0039,\n",
              "                    -0.0184, -0.0073, -0.0315, -0.0118, -0.0172, -0.0178, -0.0148, -0.0110,\n",
              "                    -0.0350, -0.0175, -0.0299, -0.0059, -0.0176, -0.0132, -0.0167, -0.0087,\n",
              "                    -0.0167, -0.0118, -0.0195, -0.0271, -0.0241, -0.0161, -0.0181, -0.0271,\n",
              "                    -0.0153, -0.0134, -0.0243, -0.0178, -0.0607, -0.0238, -0.0239, -0.0288,\n",
              "                    -0.0228, -0.0186, -0.0154, -0.0399, -0.0218, -0.0206, -0.0358, -0.0137,\n",
              "                    -0.0137, -0.0238, -0.0273, -0.0542, -0.0328, -0.0200, -0.0244, -0.0302,\n",
              "                    -0.0371, -0.0190, -0.0183, -0.0213, -0.0268, -0.0490, -0.0290, -0.0191,\n",
              "                    -0.0167, -0.0208, -0.0069, -0.0093, -0.0204, -0.0142, -0.0215, -0.0304,\n",
              "                    -0.0176, -0.0263, -0.0183, -0.0201, -0.0145, -0.0222, -0.0211, -0.0131,\n",
              "                    -0.0080, -0.0115, -0.0183, -0.0181, -0.0112, -0.0157, -0.0170, -0.0116,\n",
              "                    -0.0224, -0.0214, -0.0215, -0.0148, -0.0219, -0.0149, -0.0177, -0.0186,\n",
              "                    -0.0212, -0.0119, -0.0347, -0.0214, -0.0170, -0.0316, -0.0269, -0.0199,\n",
              "                    -0.0190, -0.0078, -0.0178, -0.0170, -0.0081, -0.0204, -0.0171, -0.0175,\n",
              "                    -0.0243, -0.0292, -0.0112, -0.0250, -0.0179, -0.0304, -0.0183, -0.0174,\n",
              "                    -0.0131, -0.0147, -0.0190, -0.0145, -0.0154, -0.0190, -0.0348, -0.0096,\n",
              "                    -0.0061, -0.0169, -0.0191, -0.0174, -0.0148, -0.0089, -0.0240, -0.0169,\n",
              "                    -0.0140, -0.0433, -0.0218, -0.0114, -0.0141, -0.0241, -0.0474, -0.0219,\n",
              "                    -0.0125, -0.0175, -0.0329, -0.0208, -0.0207, -0.0280, -0.0075, -0.0143,\n",
              "                    -0.0152, -0.0175, -0.0130, -0.0096, -0.0229, -0.0233, -0.0192, -0.0243,\n",
              "                    -0.0118, -0.0204, -0.0146, -0.0256, -0.0198, -0.0206, -0.0111, -0.0190,\n",
              "                    -0.0148, -0.0069, -0.0253, -0.0211, -0.0235, -0.0174, -0.0207, -0.0155,\n",
              "                    -0.0088, -0.0185, -0.0165, -0.0107, -0.0173, -0.0126, -0.0142, -0.0258,\n",
              "                    -0.0118, -0.0113, -0.0262, -0.0246, -0.0120, -0.0222, -0.0168, -0.0212,\n",
              "                    -0.0144, -0.0054, -0.0169, -0.0076, -0.0240, -0.0165, -0.0208, -0.0160,\n",
              "                    -0.0218, -0.0212, -0.0188, -0.0662, -0.0411, -0.0045, -0.0078, -0.0321,\n",
              "                    -0.0389, -0.0075, -0.0319, -0.0073, -0.0170, -0.0132, -0.0239, -0.0149,\n",
              "                    -0.0186, -0.0185, -0.0284, -0.0221, -0.0076, -0.0143, -0.0103, -0.0161,\n",
              "                    -0.0366, -0.0308, -0.0208, -0.0091, -0.0154, -0.0128, -0.0239, -0.0117,\n",
              "                    -0.0291, -0.0379, -0.0176, -0.0230, -0.0131, -0.0196, -0.0165, -0.0169,\n",
              "                    -0.0190, -0.0181, -0.0280, -0.0232, -0.0264, -0.0220, -0.0108, -0.0125,\n",
              "                    -0.0185, -0.0218, -0.0088, -0.0160, -0.0059, -0.0170, -0.0182, -0.0282,\n",
              "                    -0.0193, -0.0151, -0.0107, -0.0204, -0.0188, -0.0182, -0.0331, -0.0250,\n",
              "                    -0.0207, -0.0142, -0.0225, -0.0136, -0.0102, -0.0173, -0.0162, -0.0261,\n",
              "                    -0.0243, -0.0165, -0.0092, -0.0146, -0.0201, -0.0353, -0.0350, -0.0662,\n",
              "                    -0.0176, -0.0241, -0.0148, -0.0234, -0.0302, -0.0499, -0.0092, -0.0274,\n",
              "                    -0.0231, -0.0109, -0.0199, -0.0118, -0.0232, -0.0169, -0.0111, -0.0167]), max_val=tensor([0.0234, 0.0162, 0.0148, 0.0288, 0.0398, 0.0109, 0.0140, 0.0127, 0.0218,\n",
              "                    0.0173, 0.0444, 0.0380, 0.0393, 0.0301, 0.0406, 0.0067, 0.0162, 0.0200,\n",
              "                    0.0185, 0.0291, 0.0121, 0.0251, 0.0047, 0.0179, 0.0206, 0.0327, 0.0183,\n",
              "                    0.0140, 0.0203, 0.0420, 0.0375, 0.0147, 0.0206, 0.0041, 0.0186, 0.0333,\n",
              "                    0.0192, 0.0237, 0.0240, 0.0193, 0.0207, 0.0215, 0.0279, 0.0213, 0.0271,\n",
              "                    0.0281, 0.0235, 0.0247, 0.0111, 0.0585, 0.0218, 0.0162, 0.0287, 0.0225,\n",
              "                    0.0214, 0.0131, 0.0102, 0.0281, 0.0237, 0.0383, 0.0220, 0.0212, 0.0151,\n",
              "                    0.0379, 0.0201, 0.0523, 0.0117, 0.0156, 0.0419, 0.0114, 0.0195, 0.0216,\n",
              "                    0.0164, 0.0134, 0.0209, 0.0192, 0.0340, 0.0143, 0.0214, 0.0175, 0.0153,\n",
              "                    0.0256, 0.0305, 0.0144, 0.0262, 0.0436, 0.0150, 0.0197, 0.0147, 0.0215,\n",
              "                    0.0190, 0.0164, 0.0245, 0.0117, 0.0198, 0.0150, 0.0295, 0.0176, 0.0144,\n",
              "                    0.0118, 0.0192, 0.0165, 0.0190, 0.0172, 0.0181, 0.0220, 0.0196, 0.0142,\n",
              "                    0.0226, 0.0199, 0.0163, 0.0092, 0.0186, 0.0124, 0.0131, 0.0226, 0.0266,\n",
              "                    0.0107, 0.0199, 0.0092, 0.0233, 0.0269, 0.0225, 0.0155, 0.0232, 0.0078,\n",
              "                    0.0153, 0.0201, 0.0194, 0.0129, 0.0232, 0.0349, 0.0166, 0.0200, 0.0221,\n",
              "                    0.0134, 0.0219, 0.0206, 0.0305, 0.0273, 0.0124, 0.0126, 0.0147, 0.0257,\n",
              "                    0.0149, 0.0130, 0.0197, 0.0170, 0.0224, 0.0240, 0.0322, 0.0239, 0.0357,\n",
              "                    0.0244, 0.0156, 0.0287, 0.0155, 0.0247, 0.0093, 0.0173, 0.0191, 0.0203,\n",
              "                    0.0216, 0.0185, 0.0216, 0.0516, 0.0176, 0.0191, 0.0244, 0.0333, 0.0217,\n",
              "                    0.0480, 0.0158, 0.0238, 0.0148, 0.0250, 0.0213, 0.0187, 0.0511, 0.0203,\n",
              "                    0.0248, 0.0286, 0.0308, 0.0209, 0.0157, 0.0148, 0.0337, 0.0309, 0.0573,\n",
              "                    0.0307, 0.0135, 0.0305, 0.0122, 0.0128, 0.0059, 0.0110, 0.0134, 0.0121,\n",
              "                    0.0287, 0.0250, 0.0201, 0.0130, 0.0143, 0.0156, 0.0268, 0.0096, 0.0143,\n",
              "                    0.0175, 0.0201, 0.0202, 0.0185, 0.0460, 0.0095, 0.0231, 0.0123, 0.0220,\n",
              "                    0.0048, 0.0162, 0.0225, 0.0279, 0.0190, 0.0306, 0.0408, 0.0080, 0.0123,\n",
              "                    0.0128, 0.0265, 0.0146, 0.0199, 0.0211, 0.0167, 0.0145, 0.0284, 0.0134,\n",
              "                    0.0252, 0.0074, 0.0150, 0.0147, 0.0196, 0.0114, 0.0171, 0.0164, 0.0197,\n",
              "                    0.0278, 0.0244, 0.0207, 0.0182, 0.0310, 0.0163, 0.0166, 0.0211, 0.0226,\n",
              "                    0.0697, 0.0285, 0.0219, 0.0205, 0.0278, 0.0190, 0.0168, 0.0450, 0.0153,\n",
              "                    0.0153, 0.0471, 0.0121, 0.0172, 0.0309, 0.0345, 0.0501, 0.0525, 0.0377,\n",
              "                    0.0214, 0.0265, 0.0335, 0.0164, 0.0190, 0.0294, 0.0278, 0.0488, 0.0262,\n",
              "                    0.0252, 0.0203, 0.0171, 0.0105, 0.0143, 0.0226, 0.0142, 0.0186, 0.0528,\n",
              "                    0.0168, 0.0227, 0.0242, 0.0211, 0.0145, 0.0291, 0.0172, 0.0211, 0.0071,\n",
              "                    0.0124, 0.0190, 0.0138, 0.0153, 0.0170, 0.0229, 0.0111, 0.0172, 0.0239,\n",
              "                    0.0288, 0.0211, 0.0215, 0.0163, 0.0131, 0.0182, 0.0211, 0.0102, 0.0236,\n",
              "                    0.0198, 0.0173, 0.0325, 0.0315, 0.0160, 0.0126, 0.0104, 0.0165, 0.0133,\n",
              "                    0.0055, 0.0185, 0.0204, 0.0218, 0.0253, 0.0314, 0.0117, 0.0169, 0.0168,\n",
              "                    0.0287, 0.0173, 0.0178, 0.0158, 0.0278, 0.0262, 0.0178, 0.0124, 0.0186,\n",
              "                    0.0356, 0.0193, 0.0083, 0.0127, 0.0139, 0.0281, 0.0123, 0.0116, 0.0264,\n",
              "                    0.0195, 0.0184, 0.0438, 0.0288, 0.0117, 0.0122, 0.0275, 0.0513, 0.0194,\n",
              "                    0.0107, 0.0137, 0.0289, 0.0212, 0.0186, 0.0234, 0.0104, 0.0187, 0.0131,\n",
              "                    0.0153, 0.0200, 0.0100, 0.0218, 0.0198, 0.0225, 0.0203, 0.0137, 0.0278,\n",
              "                    0.0106, 0.0313, 0.0171, 0.0213, 0.0131, 0.0157, 0.0132, 0.0102, 0.0408,\n",
              "                    0.0208, 0.0236, 0.0244, 0.0291, 0.0137, 0.0156, 0.0184, 0.0252, 0.0140,\n",
              "                    0.0183, 0.0216, 0.0210, 0.0290, 0.0115, 0.0175, 0.0200, 0.0231, 0.0135,\n",
              "                    0.0222, 0.0270, 0.0177, 0.0193, 0.0073, 0.0134, 0.0108, 0.0215, 0.0217,\n",
              "                    0.0228, 0.0167, 0.0269, 0.0184, 0.0177, 0.0609, 0.0437, 0.0095, 0.0044,\n",
              "                    0.0244, 0.0404, 0.0101, 0.0346, 0.0099, 0.0243, 0.0110, 0.0245, 0.0193,\n",
              "                    0.0328, 0.0223, 0.0235, 0.0170, 0.0052, 0.0113, 0.0084, 0.0200, 0.0359,\n",
              "                    0.0269, 0.0176, 0.0136, 0.0148, 0.0161, 0.0246, 0.0139, 0.0271, 0.0348,\n",
              "                    0.0171, 0.0260, 0.0145, 0.0191, 0.0135, 0.0181, 0.0171, 0.0134, 0.0204,\n",
              "                    0.0203, 0.0280, 0.0282, 0.0071, 0.0135, 0.0237, 0.0174, 0.0116, 0.0223,\n",
              "                    0.0104, 0.0197, 0.0152, 0.0249, 0.0163, 0.0196, 0.0079, 0.0215, 0.0230,\n",
              "                    0.0188, 0.0410, 0.0178, 0.0156, 0.0154, 0.0197, 0.0176, 0.0072, 0.0089,\n",
              "                    0.0196, 0.0252, 0.0238, 0.0157, 0.0139, 0.0172, 0.0150, 0.0441, 0.0516,\n",
              "                    0.0514, 0.0160, 0.0153, 0.0145, 0.0209, 0.0308, 0.0725, 0.0149, 0.0223,\n",
              "                    0.0263, 0.0174, 0.0158, 0.0105, 0.0199, 0.0179, 0.0135, 0.0147])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0329, -0.0326, -0.0131, -0.0452, -0.0174, -0.0272, -0.0323, -0.0340,\n",
              "                    -0.0231, -0.0283, -0.0252, -0.0174, -0.0369, -0.0482, -0.0284, -0.0269,\n",
              "                    -0.0190, -0.0422, -0.0399, -0.0317, -0.0551, -0.0500, -0.0366, -0.0082,\n",
              "                    -0.0252, -0.0347, -0.0122, -0.0329, -0.0269, -0.0163, -0.0243, -0.0183,\n",
              "                    -0.0276, -0.0241, -0.0191, -0.0229, -0.0103, -0.0278, -0.0172, -0.0244,\n",
              "                    -0.0411, -0.0101, -0.0394, -0.0280, -0.0470, -0.0428, -0.0187, -0.0231,\n",
              "                    -0.0254, -0.0355, -0.0198, -0.0264, -0.0289, -0.0400, -0.0313, -0.0475,\n",
              "                    -0.0361, -0.0470, -0.0395, -0.0534, -0.0388, -0.0274, -0.0262, -0.0214,\n",
              "                    -0.0194, -0.0270, -0.0335, -0.0320, -0.0114, -0.0446, -0.0306, -0.0533,\n",
              "                    -0.0291, -0.0208, -0.0347, -0.0282, -0.0258, -0.0290, -0.0301, -0.0217,\n",
              "                    -0.0361, -0.0312, -0.0343, -0.0280, -0.0245, -0.0366, -0.0425, -0.0312,\n",
              "                    -0.0300, -0.0269, -0.0485, -0.0403, -0.0277, -0.0454, -0.0344, -0.0385,\n",
              "                    -0.0226, -0.0154, -0.0320, -0.0246, -0.0199, -0.0377, -0.0334, -0.0271,\n",
              "                    -0.0253, -0.0321, -0.0206, -0.0119, -0.0275, -0.0286, -0.0283, -0.0221,\n",
              "                    -0.0115, -0.0226, -0.0423, -0.0227, -0.0267, -0.0284, -0.0413, -0.0187,\n",
              "                    -0.0190, -0.0311, -0.0204, -0.0146, -0.0320, -0.0210, -0.0252, -0.0316,\n",
              "                    -0.0348, -0.0214, -0.0246, -0.0329, -0.0267, -0.0273, -0.0220, -0.0185,\n",
              "                    -0.0284, -0.0309, -0.0224, -0.0081, -0.0328, -0.0210, -0.0161, -0.0310,\n",
              "                    -0.0398, -0.0095, -0.0311, -0.0234, -0.0402, -0.0070, -0.0401, -0.0167,\n",
              "                    -0.0339, -0.0162, -0.0372, -0.0333, -0.0304, -0.0329, -0.0356, -0.0320,\n",
              "                    -0.0448, -0.0306, -0.0226, -0.0278, -0.0204, -0.0278, -0.0309, -0.0452,\n",
              "                    -0.0438, -0.0232, -0.0265, -0.0362, -0.0395, -0.0229, -0.0257, -0.0551,\n",
              "                    -0.0321, -0.0194, -0.0257, -0.0328, -0.0210, -0.0287, -0.0277, -0.0260,\n",
              "                    -0.0310, -0.0284, -0.0211, -0.0384, -0.0257, -0.0444, -0.0293, -0.0405,\n",
              "                    -0.0235, -0.0317, -0.0180, -0.0358, -0.0236, -0.0354, -0.0443, -0.0352,\n",
              "                    -0.0469, -0.0443, -0.0216, -0.0182, -0.0357, -0.0214, -0.0451, -0.0355,\n",
              "                    -0.0154, -0.0354, -0.0265, -0.0275, -0.0423, -0.0267, -0.0244, -0.0368,\n",
              "                    -0.0364, -0.0300, -0.0268, -0.0354, -0.0180, -0.0200, -0.0395, -0.0115,\n",
              "                    -0.0215, -0.0404, -0.0216, -0.0229, -0.0178, -0.0162, -0.0289, -0.0224,\n",
              "                    -0.0384, -0.0243, -0.0296, -0.0524, -0.0360, -0.0262, -0.0313, -0.0248,\n",
              "                    -0.0235, -0.0223, -0.0358, -0.0198, -0.0201, -0.0293, -0.0435, -0.0352,\n",
              "                    -0.0267, -0.0237, -0.0137, -0.0148, -0.0288, -0.0374, -0.0262, -0.0281,\n",
              "                    -0.0316, -0.0410, -0.0267, -0.0328, -0.0442, -0.0402, -0.0367, -0.0253,\n",
              "                    -0.0336, -0.0214, -0.0320, -0.0238, -0.0305, -0.0302, -0.0530, -0.0353,\n",
              "                    -0.0296, -0.0306, -0.0200, -0.0365, -0.0355, -0.0359, -0.0004, -0.0244,\n",
              "                    -0.0304, -0.0240, -0.0355, -0.0302, -0.0178, -0.0362, -0.0259, -0.0248,\n",
              "                    -0.0422, -0.0235, -0.0261, -0.0286, -0.0489, -0.0231, -0.0314, -0.0166,\n",
              "                    -0.0154, -0.0277, -0.0322, -0.0206, -0.0302, -0.0337, -0.0337, -0.0266,\n",
              "                    -0.0286, -0.0186, -0.0298, -0.0387, -0.0322, -0.0205, -0.0295, -0.0228,\n",
              "                    -0.0280, -0.0332, -0.0380, -0.0462, -0.0278, -0.0328, -0.0150, -0.0249,\n",
              "                    -0.0278, -0.0287, -0.0086, -0.0287, -0.0370, -0.0372, -0.0236, -0.0438,\n",
              "                    -0.0330, -0.0244, -0.0270, -0.0150, -0.0101, -0.0246, -0.0126, -0.0236,\n",
              "                    -0.0245, -0.0211, -0.0349, -0.0271, -0.0311, -0.0220, -0.0374, -0.0480,\n",
              "                    -0.0248, -0.0342, -0.0151, -0.0129, -0.0505, -0.0380, -0.0329, -0.0306,\n",
              "                    -0.0316, -0.0210, -0.0213, -0.0284, -0.0206, -0.0280, -0.0208, -0.0364,\n",
              "                    -0.0161, -0.0259, -0.0475, -0.0242, -0.0404, -0.0336, -0.0603, -0.0277,\n",
              "                    -0.0330, -0.0382, -0.0568, -0.0181, -0.0363, -0.0391, -0.0263, -0.0402,\n",
              "                    -0.0220, -0.0187, -0.0236, -0.0261, -0.0246, -0.0300, -0.0427, -0.0257,\n",
              "                    -0.0417, -0.0232, -0.0580, -0.0353, -0.0450, -0.0347, -0.0346, -0.0237,\n",
              "                    -0.0250, -0.0257, -0.0234, -0.0311, -0.0304, -0.0238, -0.0108, -0.0200,\n",
              "                    -0.0329, -0.0421, -0.0149, -0.0433, -0.0167, -0.0273, -0.0325, -0.0215,\n",
              "                    -0.0218, -0.0225, -0.0415, -0.0343, -0.0104, -0.0249, -0.0445, -0.0381,\n",
              "                    -0.0262, -0.0212, -0.0322, -0.0259, -0.0304, -0.0330, -0.0466, -0.0174,\n",
              "                    -0.0341, -0.0291, -0.0325, -0.0448, -0.0284, -0.0433, -0.0286, -0.0269,\n",
              "                    -0.0339, -0.0386, -0.0301, -0.0137, -0.0169, -0.0209, -0.0447, -0.0324,\n",
              "                    -0.0310, -0.0296, -0.0498, -0.0235, -0.0267, -0.0391, -0.0276, -0.0176,\n",
              "                    -0.0206, -0.0419, -0.0387, -0.0361, -0.0359, -0.0092, -0.0116, -0.0100,\n",
              "                    -0.0137, -0.0403, -0.0260, -0.0487, -0.0337, -0.0360, -0.0334, -0.0241,\n",
              "                    -0.0383, -0.0256, -0.0231, -0.0277, -0.0318, -0.0392, -0.0225, -0.0362,\n",
              "                    -0.0251, -0.0469, -0.0232, -0.0325, -0.0475, -0.0301, -0.0420, -0.0377,\n",
              "                    -0.0204, -0.0330, -0.0361, -0.0225, -0.0384, -0.0339, -0.0232, -0.0320,\n",
              "                    -0.0048, -0.0319, -0.0167, -0.0455, -0.0254, -0.0253, -0.0317, -0.0340,\n",
              "                    -0.0275, -0.0134, -0.0206, -0.0202, -0.0564, -0.0215, -0.0195, -0.0164,\n",
              "                    -0.0072, -0.0260, -0.0259, -0.0316, -0.0459, -0.0246, -0.0356, -0.0282]), max_val=tensor([0.0235, 0.0324, 0.0186, 0.0449, 0.0182, 0.0384, 0.0283, 0.0349, 0.0218,\n",
              "                    0.0327, 0.0287, 0.0253, 0.0341, 0.0375, 0.0256, 0.0382, 0.0216, 0.0383,\n",
              "                    0.0284, 0.0259, 0.0450, 0.0455, 0.0347, 0.0129, 0.0233, 0.0509, 0.0137,\n",
              "                    0.0287, 0.0359, 0.0234, 0.0231, 0.0153, 0.0265, 0.0265, 0.0216, 0.0304,\n",
              "                    0.0181, 0.0344, 0.0377, 0.0295, 0.0453, 0.0140, 0.0442, 0.0250, 0.0356,\n",
              "                    0.0421, 0.0119, 0.0225, 0.0264, 0.0373, 0.0184, 0.0232, 0.0328, 0.0385,\n",
              "                    0.0266, 0.0482, 0.0365, 0.0452, 0.0401, 0.0469, 0.0386, 0.0226, 0.0221,\n",
              "                    0.0223, 0.0190, 0.0270, 0.0341, 0.0312, 0.0096, 0.0532, 0.0387, 0.0338,\n",
              "                    0.0275, 0.0358, 0.0378, 0.0350, 0.0296, 0.0246, 0.0227, 0.0252, 0.0305,\n",
              "                    0.0266, 0.0192, 0.0341, 0.0318, 0.0395, 0.0345, 0.0325, 0.0287, 0.0217,\n",
              "                    0.0395, 0.0462, 0.0406, 0.0322, 0.0332, 0.0405, 0.0293, 0.0202, 0.0461,\n",
              "                    0.0283, 0.0223, 0.0514, 0.0335, 0.0318, 0.0232, 0.0380, 0.0150, 0.0156,\n",
              "                    0.0320, 0.0305, 0.0352, 0.0228, 0.0177, 0.0246, 0.0311, 0.0233, 0.0251,\n",
              "                    0.0247, 0.0366, 0.0168, 0.0229, 0.0379, 0.0288, 0.0198, 0.0556, 0.0256,\n",
              "                    0.0294, 0.0327, 0.0487, 0.0208, 0.0236, 0.0477, 0.0274, 0.0273, 0.0226,\n",
              "                    0.0175, 0.0342, 0.0327, 0.0417, 0.0154, 0.0190, 0.0318, 0.0163, 0.0257,\n",
              "                    0.0309, 0.0148, 0.0370, 0.0299, 0.0423, 0.0134, 0.0350, 0.0223, 0.0377,\n",
              "                    0.0161, 0.0302, 0.0442, 0.0290, 0.0290, 0.0282, 0.0221, 0.0549, 0.0351,\n",
              "                    0.0211, 0.0289, 0.0295, 0.0368, 0.0260, 0.0340, 0.0446, 0.0242, 0.0400,\n",
              "                    0.0375, 0.0333, 0.0201, 0.0344, 0.0395, 0.0357, 0.0219, 0.0270, 0.0371,\n",
              "                    0.0185, 0.0310, 0.0330, 0.0266, 0.0277, 0.0314, 0.0270, 0.0350, 0.0247,\n",
              "                    0.0335, 0.0360, 0.0266, 0.0176, 0.0315, 0.0187, 0.0334, 0.0228, 0.0371,\n",
              "                    0.0493, 0.0273, 0.0514, 0.0398, 0.0264, 0.0153, 0.0382, 0.0141, 0.0448,\n",
              "                    0.0311, 0.0114, 0.0347, 0.0240, 0.0314, 0.0264, 0.0352, 0.0301, 0.0436,\n",
              "                    0.0311, 0.0270, 0.0265, 0.0395, 0.0241, 0.0296, 0.0334, 0.0147, 0.0146,\n",
              "                    0.0316, 0.0141, 0.0265, 0.0234, 0.0254, 0.0291, 0.0355, 0.0438, 0.0243,\n",
              "                    0.0311, 0.0511, 0.0291, 0.0287, 0.0347, 0.0492, 0.0525, 0.0230, 0.0383,\n",
              "                    0.0285, 0.0273, 0.0338, 0.0366, 0.0310, 0.0210, 0.0227, 0.0254, 0.0183,\n",
              "                    0.0320, 0.0387, 0.0286, 0.0524, 0.0264, 0.0441, 0.0255, 0.0587, 0.0379,\n",
              "                    0.0434, 0.0398, 0.0323, 0.0353, 0.0282, 0.0288, 0.0225, 0.0289, 0.0597,\n",
              "                    0.0424, 0.0430, 0.0303, 0.0248, 0.0163, 0.0289, 0.0249, 0.0329, 0.0003,\n",
              "                    0.0265, 0.0415, 0.0285, 0.0398, 0.0320, 0.0186, 0.0340, 0.0280, 0.0186,\n",
              "                    0.0442, 0.0269, 0.0213, 0.0338, 0.0374, 0.0175, 0.0282, 0.0165, 0.0249,\n",
              "                    0.0214, 0.0315, 0.0161, 0.0651, 0.0263, 0.0421, 0.0285, 0.0404, 0.0175,\n",
              "                    0.0284, 0.0508, 0.0433, 0.0133, 0.0299, 0.0194, 0.0322, 0.0449, 0.0406,\n",
              "                    0.0601, 0.0301, 0.0299, 0.0255, 0.0320, 0.0249, 0.0369, 0.0137, 0.0303,\n",
              "                    0.0280, 0.0720, 0.0204, 0.0353, 0.0285, 0.0287, 0.0281, 0.0101, 0.0104,\n",
              "                    0.0342, 0.0086, 0.0452, 0.0192, 0.0248, 0.0356, 0.0217, 0.0457, 0.0193,\n",
              "                    0.0449, 0.0376, 0.0223, 0.0326, 0.0199, 0.0083, 0.0350, 0.0379, 0.0279,\n",
              "                    0.0209, 0.0324, 0.0222, 0.0206, 0.0330, 0.0242, 0.0351, 0.0268, 0.0348,\n",
              "                    0.0246, 0.0399, 0.0489, 0.0326, 0.0390, 0.0296, 0.0723, 0.0296, 0.0311,\n",
              "                    0.0402, 0.0685, 0.0164, 0.0387, 0.0429, 0.0325, 0.0357, 0.0198, 0.0214,\n",
              "                    0.0201, 0.0213, 0.0239, 0.0363, 0.0362, 0.0221, 0.0483, 0.0283, 0.0563,\n",
              "                    0.0310, 0.0477, 0.0245, 0.0321, 0.0301, 0.0259, 0.0188, 0.0205, 0.0351,\n",
              "                    0.0344, 0.0172, 0.0065, 0.0213, 0.0283, 0.0276, 0.0170, 0.0320, 0.0156,\n",
              "                    0.0222, 0.0297, 0.0240, 0.0284, 0.0300, 0.0418, 0.0285, 0.0215, 0.0274,\n",
              "                    0.0369, 0.0317, 0.0300, 0.0312, 0.0542, 0.0361, 0.0307, 0.0252, 0.0457,\n",
              "                    0.0198, 0.0301, 0.0345, 0.0279, 0.0299, 0.0352, 0.0337, 0.0253, 0.0388,\n",
              "                    0.0327, 0.0299, 0.0237, 0.0141, 0.0248, 0.0326, 0.0441, 0.0384, 0.0271,\n",
              "                    0.0330, 0.0290, 0.0333, 0.0232, 0.0363, 0.0222, 0.0112, 0.0168, 0.0405,\n",
              "                    0.0345, 0.0449, 0.0452, 0.0115, 0.0190, 0.0137, 0.0098, 0.0344, 0.0305,\n",
              "                    0.0567, 0.0363, 0.0525, 0.0410, 0.0174, 0.0264, 0.0327, 0.0350, 0.0283,\n",
              "                    0.0308, 0.0350, 0.0198, 0.0375, 0.0284, 0.0407, 0.0238, 0.0393, 0.0503,\n",
              "                    0.0300, 0.0491, 0.0461, 0.0226, 0.0402, 0.0373, 0.0217, 0.0255, 0.0300,\n",
              "                    0.0415, 0.0468, 0.0093, 0.0402, 0.0156, 0.0420, 0.0503, 0.0287, 0.0281,\n",
              "                    0.0403, 0.0277, 0.0092, 0.0219, 0.0169, 0.0535, 0.0223, 0.0255, 0.0201,\n",
              "                    0.0127, 0.0338, 0.0242, 0.0316, 0.0322, 0.0242, 0.0306, 0.0327])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (downsample): Sequential(\n",
              "          (0): ConvBn2d(\n",
              "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
              "            (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "              min_val=tensor([-0.0499, -0.0211, -0.0530, -0.0393, -0.0407, -0.0406, -0.0344, -0.0546,\n",
              "                      -0.0432, -0.0374, -0.0432, -0.0625, -0.0495, -0.0514, -0.0245, -0.0371,\n",
              "                      -0.0362, -0.0378, -0.0368, -0.0453, -0.0404, -0.0584, -0.0458, -0.0420,\n",
              "                      -0.0302, -0.0319, -0.0453, -0.0322, -0.0449, -0.0413, -0.0409, -0.0315,\n",
              "                      -0.0598, -0.0483, -0.0542, -0.0604, -0.0444, -0.0333, -0.0376, -0.0494,\n",
              "                      -0.0567, -0.0256, -0.0412, -0.0443, -0.0487, -0.0409, -0.0460, -0.0414,\n",
              "                      -0.0289, -0.0346, -0.0340, -0.0312, -0.0158, -0.0404, -0.0359, -0.0389,\n",
              "                      -0.0400, -0.0429, -0.0550, -0.0491, -0.0581, -0.0507, -0.0354, -0.0418,\n",
              "                      -0.0301, -0.0384, -0.0429, -0.0695, -0.0478, -0.0259, -0.0461, -0.0369,\n",
              "                      -0.0396, -0.0430, -0.0325, -0.0375, -0.0574, -0.0336, -0.0484, -0.0483,\n",
              "                      -0.0507, -0.0343, -0.0336, -0.0271, -0.0334, -0.0236, -0.0465, -0.0400,\n",
              "                      -0.0341, -0.0524, -0.0524, -0.0326, -0.0252, -0.0366, -0.0518, -0.0253,\n",
              "                      -0.0446, -0.0339, -0.0348, -0.0250, -0.0473, -0.0344, -0.0157, -0.0356,\n",
              "                      -0.0501, -0.0396, -0.0393, -0.0226, -0.0412, -0.0322, -0.0520, -0.0423,\n",
              "                      -0.0468, -0.0247, -0.0491, -0.0334, -0.0370, -0.0403, -0.0219, -0.0521,\n",
              "                      -0.0254, -0.0308, -0.0239, -0.0374, -0.0339, -0.0342, -0.0440, -0.0381,\n",
              "                      -0.0503, -0.0418, -0.0345, -0.0865, -0.0446, -0.0387, -0.0503, -0.0436,\n",
              "                      -0.0273, -0.0518, -0.0443, -0.0538, -0.0270, -0.0331, -0.0596, -0.0324,\n",
              "                      -0.0365, -0.0182, -0.0276, -0.0418, -0.0320, -0.0255, -0.0213, -0.0274,\n",
              "                      -0.0328, -0.0400, -0.0289, -0.0600, -0.0496, -0.0423, -0.0246, -0.0406,\n",
              "                      -0.0546, -0.0274, -0.0434, -0.0314, -0.0332, -0.0431, -0.0744, -0.0291,\n",
              "                      -0.0371, -0.0306, -0.0437, -0.0377, -0.0673, -0.0295, -0.0412, -0.0516,\n",
              "                      -0.0710, -0.0091, -0.0172, -0.0542, -0.0503, -0.0550, -0.0313, -0.0411,\n",
              "                      -0.0330, -0.0598, -0.0448, -0.0360, -0.0331, -0.0352, -0.0530, -0.0395,\n",
              "                      -0.0353, -0.0591, -0.0397, -0.0222, -0.0572, -0.0406, -0.0111, -0.0456,\n",
              "                      -0.0325, -0.0763, -0.0259, -0.0235, -0.0451, -0.0218, -0.0387, -0.0309,\n",
              "                      -0.0142, -0.0295, -0.0553, -0.0293, -0.0195, -0.0549, -0.0248, -0.0942,\n",
              "                      -0.0526, -0.0146, -0.0298, -0.0364, -0.0301, -0.0437, -0.0375, -0.0246,\n",
              "                      -0.0319, -0.0447, -0.0477, -0.0335, -0.0665, -0.0145, -0.0366, -0.0569,\n",
              "                      -0.0420, -0.0244, -0.0339, -0.0172, -0.0474, -0.0486, -0.0528, -0.0261,\n",
              "                      -0.0216, -0.0689, -0.0515, -0.0165, -0.0257, -0.0339, -0.0380, -0.0386,\n",
              "                      -0.0436, -0.0414, -0.0479, -0.0559, -0.0573, -0.0205, -0.0442, -0.0614,\n",
              "                      -0.0346, -0.0399, -0.0736, -0.0418, -0.0493, -0.0561, -0.0217, -0.0630,\n",
              "                      -0.0479, -0.0212, -0.0318, -0.0280, -0.0413, -0.0237, -0.0495, -0.0205,\n",
              "                      -0.0266, -0.0540, -0.0328, -0.0468, -0.0278, -0.0439, -0.0004, -0.0314,\n",
              "                      -0.0313, -0.0531, -0.0321, -0.0496, -0.0279, -0.0506, -0.0358, -0.0185,\n",
              "                      -0.0737, -0.0751, -0.0421, -0.0343, -0.0535, -0.0276, -0.0344, -0.0226,\n",
              "                      -0.0722, -0.0311, -0.0424, -0.0316, -0.0282, -0.0435, -0.0420, -0.0369,\n",
              "                      -0.0216, -0.0477, -0.0267, -0.0274, -0.0450, -0.0304, -0.0407, -0.0538,\n",
              "                      -0.0521, -0.0407, -0.0289, -0.0346, -0.0186, -0.0286, -0.0324, -0.0632,\n",
              "                      -0.0256, -0.0467, -0.0618, -0.0422, -0.0310, -0.0369, -0.0405, -0.0440,\n",
              "                      -0.0355, -0.0122, -0.0400, -0.0804, -0.0386, -0.0381, -0.0522, -0.0261,\n",
              "                      -0.0212, -0.0394, -0.0209, -0.0645, -0.0592, -0.0404, -0.0663, -0.0385,\n",
              "                      -0.0402, -0.0429, -0.0450, -0.0243, -0.0444, -0.0506, -0.0544, -0.0607,\n",
              "                      -0.0594, -0.0532, -0.0401, -0.0308, -0.0460, -0.0257, -0.0617, -0.0268,\n",
              "                      -0.0463, -0.0486, -0.0334, -0.0179, -0.0770, -0.0339, -0.0436, -0.0278,\n",
              "                      -0.0231, -0.0295, -0.0576, -0.0201, -0.0384, -0.0494, -0.0387, -0.0465,\n",
              "                      -0.0253, -0.0402, -0.0325, -0.0402, -0.0240, -0.0349, -0.0321, -0.0337,\n",
              "                      -0.0202, -0.0493, -0.0487, -0.0373, -0.0267, -0.0373, -0.0428, -0.0349,\n",
              "                      -0.0294, -0.0376, -0.0389, -0.0427, -0.0410, -0.0533, -0.0375, -0.0473,\n",
              "                      -0.0448, -0.0333, -0.0387, -0.0637, -0.0192, -0.0348, -0.0395, -0.0299,\n",
              "                      -0.0564, -0.0360, -0.0290, -0.0462, -0.0215, -0.0322, -0.0601, -0.0450,\n",
              "                      -0.0555, -0.0362, -0.0257, -0.0285, -0.0567, -0.0412, -0.0270, -0.0261,\n",
              "                      -0.0393, -0.0364, -0.0377, -0.0444, -0.0272, -0.0385, -0.0355, -0.0349,\n",
              "                      -0.0243, -0.0497, -0.0520, -0.0333, -0.0591, -0.0371, -0.0540, -0.0274,\n",
              "                      -0.0298, -0.0373, -0.0331, -0.0239, -0.0391, -0.0548, -0.0233, -0.0254,\n",
              "                      -0.0409, -0.0170, -0.0363, -0.0238, -0.0441, -0.0183, -0.0529, -0.0658,\n",
              "                      -0.0347, -0.0160, -0.0475, -0.0491, -0.0394, -0.0294, -0.0284, -0.0365,\n",
              "                      -0.0418, -0.0361, -0.0454, -0.0345, -0.0340, -0.0260, -0.0483, -0.0505,\n",
              "                      -0.0534, -0.0324, -0.0403, -0.0548, -0.0420, -0.0385, -0.0434, -0.0442,\n",
              "                      -0.0261, -0.0188, -0.0577, -0.0533, -0.0309, -0.0463, -0.0493, -0.0352,\n",
              "                      -0.0430, -0.0314, -0.0187, -0.0295, -0.0478, -0.0346, -0.0224, -0.0541,\n",
              "                      -0.0180, -0.0210, -0.0698, -0.0349, -0.0279, -0.0427, -0.0289, -0.0519,\n",
              "                      -0.0099, -0.0303, -0.0305, -0.0330, -0.0425, -0.0499, -0.0397, -0.0217]), max_val=tensor([0.0587, 0.0338, 0.0362, 0.0410, 0.0348, 0.0353, 0.0361, 0.0426, 0.0396,\n",
              "                      0.0496, 0.0359, 0.0464, 0.0401, 0.0456, 0.0319, 0.0332, 0.0360, 0.0389,\n",
              "                      0.0330, 0.0467, 0.0417, 0.0393, 0.0446, 0.0538, 0.0471, 0.0529, 0.0338,\n",
              "                      0.0542, 0.0377, 0.0550, 0.0457, 0.0477, 0.0536, 0.0307, 0.0550, 0.0725,\n",
              "                      0.0609, 0.0367, 0.0575, 0.0371, 0.0503, 0.0228, 0.0373, 0.0636, 0.0870,\n",
              "                      0.0461, 0.0323, 0.0336, 0.0402, 0.0237, 0.0337, 0.0339, 0.0071, 0.0548,\n",
              "                      0.0435, 0.0387, 0.0463, 0.0415, 0.0478, 0.0370, 0.0446, 0.0361, 0.0373,\n",
              "                      0.0364, 0.0378, 0.0510, 0.0441, 0.0469, 0.0783, 0.0321, 0.0420, 0.0414,\n",
              "                      0.0558, 0.0272, 0.0407, 0.0322, 0.0472, 0.0364, 0.0466, 0.0478, 0.0381,\n",
              "                      0.0288, 0.0236, 0.0389, 0.0306, 0.0326, 0.0466, 0.0428, 0.0406, 0.0427,\n",
              "                      0.0591, 0.0396, 0.0368, 0.0395, 0.0486, 0.0202, 0.0564, 0.0468, 0.0359,\n",
              "                      0.0220, 0.0509, 0.0362, 0.0184, 0.0246, 0.0525, 0.0360, 0.0430, 0.0186,\n",
              "                      0.0467, 0.0196, 0.0374, 0.0314, 0.0341, 0.0303, 0.0550, 0.0342, 0.0331,\n",
              "                      0.0440, 0.0308, 0.0671, 0.0306, 0.0327, 0.0152, 0.0379, 0.0248, 0.0271,\n",
              "                      0.0347, 0.0645, 0.0368, 0.0366, 0.0284, 0.0534, 0.0486, 0.0480, 0.0499,\n",
              "                      0.0364, 0.0287, 0.0436, 0.0573, 0.0415, 0.0334, 0.0308, 0.0650, 0.0351,\n",
              "                      0.0437, 0.0103, 0.0310, 0.0527, 0.0373, 0.0266, 0.0224, 0.0316, 0.0520,\n",
              "                      0.0343, 0.0406, 0.0587, 0.0399, 0.0369, 0.0284, 0.0457, 0.0480, 0.0277,\n",
              "                      0.0793, 0.0321, 0.0339, 0.0314, 0.0518, 0.0431, 0.0382, 0.0337, 0.0317,\n",
              "                      0.0379, 0.0589, 0.0468, 0.0299, 0.0437, 0.0565, 0.0141, 0.0091, 0.0439,\n",
              "                      0.0625, 0.0487, 0.0253, 0.0465, 0.0385, 0.0610, 0.0354, 0.0478, 0.0326,\n",
              "                      0.0406, 0.0566, 0.0354, 0.0314, 0.0401, 0.0313, 0.0219, 0.0487, 0.0512,\n",
              "                      0.0219, 0.0389, 0.0392, 0.0676, 0.0564, 0.0335, 0.0527, 0.0300, 0.0391,\n",
              "                      0.0353, 0.0111, 0.0271, 0.0550, 0.0334, 0.0300, 0.0414, 0.0379, 0.0777,\n",
              "                      0.0447, 0.0199, 0.0474, 0.0284, 0.0307, 0.0441, 0.0448, 0.0235, 0.0405,\n",
              "                      0.0397, 0.0444, 0.0245, 0.1012, 0.0076, 0.0400, 0.0638, 0.0498, 0.0407,\n",
              "                      0.0440, 0.0241, 0.0510, 0.0347, 0.0509, 0.0391, 0.0099, 0.0406, 0.0456,\n",
              "                      0.0203, 0.0386, 0.0254, 0.0527, 0.0393, 0.0404, 0.0300, 0.0428, 0.0465,\n",
              "                      0.0627, 0.0304, 0.0421, 0.0462, 0.0316, 0.0542, 0.0539, 0.0416, 0.0298,\n",
              "                      0.0943, 0.0248, 0.0384, 0.0423, 0.0169, 0.0436, 0.0551, 0.0495, 0.0309,\n",
              "                      0.0352, 0.0117, 0.0201, 0.0380, 0.0635, 0.0386, 0.0191, 0.0487, 0.0003,\n",
              "                      0.0297, 0.0323, 0.0688, 0.0339, 0.0501, 0.0210, 0.0733, 0.0329, 0.0296,\n",
              "                      0.0600, 0.0685, 0.0470, 0.0512, 0.0401, 0.0253, 0.0392, 0.0128, 0.0653,\n",
              "                      0.0347, 0.0739, 0.0473, 0.0487, 0.0475, 0.0364, 0.0498, 0.0190, 0.0345,\n",
              "                      0.0224, 0.0369, 0.0375, 0.0283, 0.0371, 0.0429, 0.0336, 0.0391, 0.0414,\n",
              "                      0.0228, 0.0311, 0.0475, 0.0551, 0.0557, 0.0319, 0.0494, 0.0563, 0.0392,\n",
              "                      0.0250, 0.0330, 0.0401, 0.0507, 0.0413, 0.0065, 0.0681, 0.1042, 0.0404,\n",
              "                      0.0400, 0.0510, 0.0249, 0.0300, 0.0455, 0.0101, 0.0692, 0.0500, 0.0510,\n",
              "                      0.0675, 0.0693, 0.0395, 0.0454, 0.0454, 0.0139, 0.0508, 0.0453, 0.0505,\n",
              "                      0.0927, 0.0408, 0.0455, 0.0540, 0.0390, 0.0303, 0.0246, 0.0356, 0.0322,\n",
              "                      0.0398, 0.0368, 0.0408, 0.0083, 0.0889, 0.0406, 0.0382, 0.0297, 0.0317,\n",
              "                      0.0321, 0.0561, 0.0278, 0.0336, 0.0588, 0.0393, 0.0406, 0.0293, 0.0375,\n",
              "                      0.0431, 0.0354, 0.0351, 0.0392, 0.0364, 0.0429, 0.0222, 0.0515, 0.0519,\n",
              "                      0.0450, 0.0451, 0.0433, 0.0459, 0.0310, 0.0331, 0.0724, 0.0527, 0.0428,\n",
              "                      0.0296, 0.0688, 0.0385, 0.0408, 0.0344, 0.0427, 0.0393, 0.0699, 0.0437,\n",
              "                      0.0333, 0.0536, 0.0272, 0.0447, 0.0337, 0.0327, 0.0585, 0.0125, 0.0335,\n",
              "                      0.0601, 0.0484, 0.0408, 0.0295, 0.0393, 0.0335, 0.0429, 0.0447, 0.0333,\n",
              "                      0.0355, 0.0378, 0.0317, 0.0355, 0.0353, 0.0360, 0.0428, 0.0408, 0.0270,\n",
              "                      0.0222, 0.0392, 0.0424, 0.0334, 0.0786, 0.0388, 0.0521, 0.0509, 0.0678,\n",
              "                      0.0392, 0.0416, 0.0188, 0.0448, 0.0591, 0.0175, 0.0252, 0.0402, 0.0252,\n",
              "                      0.0410, 0.0198, 0.0444, 0.0239, 0.0738, 0.0666, 0.0421, 0.0230, 0.0381,\n",
              "                      0.0480, 0.0327, 0.0297, 0.0246, 0.0467, 0.0493, 0.0449, 0.0517, 0.0406,\n",
              "                      0.0354, 0.0506, 0.0441, 0.0343, 0.0486, 0.0414, 0.0378, 0.0512, 0.0441,\n",
              "                      0.0410, 0.0467, 0.0420, 0.0230, 0.0215, 0.0567, 0.0516, 0.0397, 0.0419,\n",
              "                      0.0407, 0.0348, 0.0382, 0.0268, 0.0285, 0.0361, 0.0380, 0.0237, 0.0277,\n",
              "                      0.0532, 0.0122, 0.0140, 0.0572, 0.0431, 0.0372, 0.0421, 0.0385, 0.0472,\n",
              "                      0.0195, 0.0401, 0.0489, 0.0373, 0.0624, 0.0378, 0.0424, 0.0201])\n",
              "            )\n",
              "            (activation_post_process): HistogramObserver()\n",
              "          )\n",
              "          (1): Identity()\n",
              "        )\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): ConvBnReLU2d(\n",
              "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-0.0990, -0.0918, -0.1316, -0.1048, -0.0980, -0.0894, -0.1008, -0.1338,\n",
              "                    -0.1246, -0.0889, -0.1070, -0.0944, -0.1538, -0.1494, -0.1269, -0.1030,\n",
              "                    -0.0936, -0.1424, -0.1227, -0.0806, -0.1699, -0.1033, -0.1037, -0.1015,\n",
              "                    -0.1449, -0.0897, -0.1165, -0.0817, -0.1228, -0.0877, -0.0965, -0.1167,\n",
              "                    -0.1248, -0.1092, -0.1234, -0.1167, -0.1105, -0.0783, -0.1132, -0.0922,\n",
              "                    -0.0923, -0.1135, -0.0881, -0.1036, -0.0957, -0.1326, -0.1482, -0.1451,\n",
              "                    -0.1327, -0.1683, -0.0989, -0.1160, -0.1180, -0.0826, -0.1323, -0.0875,\n",
              "                    -0.1456, -0.0847, -0.1053, -0.1240, -0.1066, -0.0815, -0.1058, -0.1118,\n",
              "                    -0.0975, -0.0824, -0.0712, -0.1265, -0.1212, -0.0909, -0.1086, -0.1132,\n",
              "                    -0.1119, -0.2130, -0.1697, -0.0869, -0.0963, -0.0773, -0.0771, -0.1165,\n",
              "                    -0.1002, -0.1274, -0.1136, -0.1065, -0.0913, -0.1029, -0.1334, -0.0847,\n",
              "                    -0.1202, -0.1098, -0.1138, -0.1052, -0.0769, -0.1703, -0.1256, -0.1046,\n",
              "                    -0.0832, -0.1139, -0.0981, -0.0766, -0.1295, -0.1206, -0.0937, -0.1498,\n",
              "                    -0.1281, -0.0824, -0.0836, -0.0955, -0.1579, -0.2033, -0.0621, -0.1342,\n",
              "                    -0.1489, -0.0755, -0.1456, -0.0887, -0.1578, -0.0987, -0.1327, -0.0993,\n",
              "                    -0.0763, -0.1274, -0.0944, -0.0768, -0.1205, -0.1073, -0.1103, -0.0976,\n",
              "                    -0.1098, -0.1122, -0.0845, -0.0959, -0.1140, -0.0872, -0.1300, -0.0724,\n",
              "                    -0.1216, -0.1256, -0.0845, -0.1093, -0.0826, -0.1215, -0.0823, -0.0982,\n",
              "                    -0.0803, -0.1095, -0.1228, -0.1469, -0.0914, -0.0995, -0.1363, -0.1152,\n",
              "                    -0.0546, -0.1900, -0.0975, -0.0963, -0.1022, -0.1124, -0.0988, -0.0798,\n",
              "                    -0.0860, -0.1301, -0.0995, -0.0792, -0.0705, -0.1859, -0.1254, -0.1543,\n",
              "                    -0.1020, -0.1344, -0.1489, -0.1286, -0.1146, -0.0693, -0.0933, -0.0803,\n",
              "                    -0.1606, -0.1074, -0.1822, -0.1186, -0.0727, -0.1038, -0.1311, -0.1118,\n",
              "                    -0.1315, -0.0935, -0.0984, -0.0894, -0.1126, -0.0803, -0.0865, -0.1038,\n",
              "                    -0.0757, -0.1398, -0.0802, -0.0949, -0.1899, -0.1008, -0.0993, -0.1371,\n",
              "                    -0.1054, -0.0948, -0.1060, -0.0779, -0.0885, -0.0617, -0.1552, -0.1147,\n",
              "                    -0.1163, -0.0819, -0.1203, -0.0932, -0.1038, -0.0858, -0.1122, -0.0944,\n",
              "                    -0.0984, -0.1129, -0.0843, -0.0861, -0.1061, -0.1424, -0.1166, -0.0909,\n",
              "                    -0.1309, -0.1175, -0.1037, -0.1190, -0.0994, -0.0868, -0.0824, -0.1102,\n",
              "                    -0.1095, -0.0866, -0.1363, -0.0917, -0.0903, -0.1289, -0.0957, -0.1155,\n",
              "                    -0.0990, -0.0927, -0.1020, -0.1193, -0.0914, -0.1216, -0.0948, -0.1319,\n",
              "                    -0.0715, -0.1144, -0.0892, -0.0982, -0.0805, -0.1144, -0.1232, -0.1614,\n",
              "                    -0.1219, -0.1267, -0.0808, -0.0919, -0.0763, -0.0902, -0.0804, -0.1207,\n",
              "                    -0.1152, -0.1309, -0.0741, -0.1027, -0.1492, -0.0894, -0.1442, -0.1008,\n",
              "                    -0.1257, -0.1411, -0.0804, -0.1136, -0.0947, -0.1609, -0.1307, -0.1491,\n",
              "                    -0.0809, -0.0981, -0.0821, -0.1154, -0.2009, -0.1103, -0.1262, -0.1347,\n",
              "                    -0.0982, -0.1690, -0.0971, -0.0928, -0.1321, -0.1158, -0.1177, -0.1206,\n",
              "                    -0.1019, -0.0800, -0.0864, -0.0853, -0.1133, -0.1083, -0.1608, -0.1912,\n",
              "                    -0.1551, -0.1337, -0.0868, -0.1470, -0.1430, -0.1252, -0.1665, -0.0690,\n",
              "                    -0.0999, -0.0931, -0.0860, -0.0983, -0.1045, -0.0865, -0.0932, -0.1411,\n",
              "                    -0.0874, -0.1347, -0.1215, -0.1006, -0.1010, -0.1310, -0.0978, -0.1030,\n",
              "                    -0.1080, -0.1108, -0.0870, -0.1127, -0.1263, -0.1155, -0.1255, -0.1031,\n",
              "                    -0.0986, -0.0753, -0.1264, -0.1284, -0.1142, -0.1399, -0.0689, -0.1132,\n",
              "                    -0.0937, -0.0988, -0.1165, -0.0872, -0.1513, -0.1376, -0.1027, -0.0696,\n",
              "                    -0.1256, -0.1082, -0.0780, -0.1526, -0.1078, -0.0857, -0.0999, -0.1363,\n",
              "                    -0.1250, -0.0863, -0.0757, -0.0836, -0.1038, -0.0976, -0.1162, -0.1057,\n",
              "                    -0.1008, -0.1219, -0.1141, -0.1094, -0.0834, -0.1015, -0.0803, -0.1080,\n",
              "                    -0.1457, -0.1163, -0.0674, -0.1346, -0.0889, -0.0931, -0.1206, -0.1072,\n",
              "                    -0.0987, -0.1363, -0.1561, -0.0984, -0.1311, -0.0981, -0.1065, -0.0978,\n",
              "                    -0.1364, -0.0896, -0.1159, -0.0924, -0.1221, -0.0828, -0.1053, -0.0918,\n",
              "                    -0.1453, -0.1253, -0.1075, -0.1030, -0.1171, -0.1066, -0.0670, -0.0743,\n",
              "                    -0.0917, -0.1169, -0.0973, -0.1014, -0.0957, -0.1066, -0.0698, -0.1037,\n",
              "                    -0.1094, -0.1024, -0.1094, -0.1547, -0.1620, -0.1048, -0.0761, -0.1069,\n",
              "                    -0.1319, -0.1003, -0.1214, -0.1061, -0.1007, -0.1743, -0.1170, -0.0964,\n",
              "                    -0.0916, -0.1602, -0.0900, -0.0932, -0.1810, -0.0854, -0.1553, -0.0742,\n",
              "                    -0.1572, -0.0835, -0.0964, -0.1140, -0.1304, -0.1033, -0.0745, -0.1123,\n",
              "                    -0.1164, -0.0729, -0.1137, -0.1374, -0.1209, -0.0881, -0.0904, -0.1059,\n",
              "                    -0.0687, -0.1119, -0.1367, -0.0814, -0.1125, -0.1370, -0.1289, -0.1208,\n",
              "                    -0.1013, -0.0741, -0.1259, -0.1021, -0.0870, -0.1302, -0.1383, -0.0979,\n",
              "                    -0.1253, -0.1065, -0.1352, -0.1168, -0.0892, -0.1631, -0.0999, -0.1272,\n",
              "                    -0.1548, -0.0885, -0.1077, -0.1101, -0.1446, -0.1249, -0.0975, -0.0671,\n",
              "                    -0.1239, -0.1313, -0.0590, -0.0731, -0.1116, -0.0914, -0.0960, -0.0970,\n",
              "                    -0.1057, -0.1638, -0.1131, -0.1480, -0.1376, -0.0942, -0.0929, -0.0858,\n",
              "                    -0.1630, -0.1171, -0.1734, -0.0999, -0.1532, -0.1035, -0.1290, -0.1536]), max_val=tensor([0.0756, 0.0960, 0.1009, 0.1541, 0.0829, 0.0947, 0.0654, 0.0957, 0.0876,\n",
              "                    0.0792, 0.0985, 0.1255, 0.1286, 0.1575, 0.1140, 0.1295, 0.1747, 0.1164,\n",
              "                    0.1196, 0.0821, 0.1227, 0.1454, 0.1243, 0.0966, 0.1121, 0.1680, 0.0958,\n",
              "                    0.1143, 0.0876, 0.1269, 0.0855, 0.0821, 0.0852, 0.1437, 0.0857, 0.1766,\n",
              "                    0.0809, 0.0971, 0.1367, 0.1362, 0.0653, 0.1077, 0.1249, 0.0971, 0.0652,\n",
              "                    0.1028, 0.1478, 0.1238, 0.1779, 0.1562, 0.1077, 0.1323, 0.1068, 0.1092,\n",
              "                    0.1453, 0.0861, 0.1515, 0.1263, 0.1052, 0.1803, 0.0751, 0.0837, 0.1502,\n",
              "                    0.1890, 0.1437, 0.0450, 0.1038, 0.0966, 0.0796, 0.1155, 0.0886, 0.0719,\n",
              "                    0.0894, 0.1927, 0.1579, 0.1066, 0.1030, 0.0720, 0.0847, 0.0894, 0.0914,\n",
              "                    0.1177, 0.1478, 0.1427, 0.0929, 0.1227, 0.1512, 0.1172, 0.0919, 0.1139,\n",
              "                    0.1038, 0.0747, 0.0967, 0.1453, 0.1041, 0.0855, 0.0858, 0.0956, 0.1260,\n",
              "                    0.0575, 0.1256, 0.1236, 0.0854, 0.1036, 0.1944, 0.1245, 0.1013, 0.0851,\n",
              "                    0.1769, 0.1881, 0.0913, 0.1004, 0.0847, 0.0754, 0.1011, 0.1394, 0.1024,\n",
              "                    0.0725, 0.1360, 0.1126, 0.1070, 0.0795, 0.1744, 0.1058, 0.0903, 0.0829,\n",
              "                    0.1325, 0.1201, 0.1317, 0.0910, 0.1027, 0.0740, 0.0939, 0.0913, 0.1502,\n",
              "                    0.0863, 0.1713, 0.0995, 0.0860, 0.1166, 0.0896, 0.0886, 0.0568, 0.1403,\n",
              "                    0.0754, 0.1007, 0.1034, 0.1314, 0.1155, 0.1123, 0.0952, 0.1256, 0.0850,\n",
              "                    0.2218, 0.1674, 0.1868, 0.1382, 0.0798, 0.1061, 0.1069, 0.0411, 0.1362,\n",
              "                    0.1381, 0.0889, 0.1077, 0.1653, 0.0983, 0.2614, 0.0928, 0.1833, 0.1733,\n",
              "                    0.1986, 0.1173, 0.0724, 0.1644, 0.0982, 0.1416, 0.1140, 0.1765, 0.0734,\n",
              "                    0.1105, 0.0910, 0.1184, 0.1231, 0.1645, 0.0821, 0.1071, 0.0876, 0.0816,\n",
              "                    0.0955, 0.1184, 0.0835, 0.0542, 0.1564, 0.0973, 0.0809, 0.1407, 0.0752,\n",
              "                    0.0886, 0.3204, 0.0937, 0.0996, 0.1760, 0.1283, 0.0814, 0.1064, 0.1166,\n",
              "                    0.1311, 0.1397, 0.0831, 0.1187, 0.0783, 0.1127, 0.0726, 0.1302, 0.1205,\n",
              "                    0.0748, 0.0823, 0.1196, 0.1050, 0.0898, 0.1415, 0.0642, 0.0798, 0.1192,\n",
              "                    0.0986, 0.1076, 0.1468, 0.1060, 0.0841, 0.0926, 0.1056, 0.1700, 0.0807,\n",
              "                    0.1692, 0.1094, 0.1500, 0.1566, 0.1257, 0.0982, 0.0770, 0.1726, 0.0675,\n",
              "                    0.1292, 0.1787, 0.0952, 0.1294, 0.1037, 0.1371, 0.1677, 0.1129, 0.1154,\n",
              "                    0.1011, 0.1137, 0.1562, 0.1635, 0.0892, 0.0992, 0.1464, 0.0913, 0.0514,\n",
              "                    0.1107, 0.1153, 0.0761, 0.1066, 0.0979, 0.0673, 0.0783, 0.1537, 0.0654,\n",
              "                    0.1664, 0.1371, 0.0865, 0.1146, 0.1070, 0.1012, 0.1442, 0.1081, 0.1128,\n",
              "                    0.1040, 0.0740, 0.0906, 0.0741, 0.1284, 0.1566, 0.0582, 0.1046, 0.0725,\n",
              "                    0.0724, 0.1054, 0.0775, 0.1338, 0.1857, 0.0650, 0.0773, 0.0918, 0.1040,\n",
              "                    0.1620, 0.1229, 0.1243, 0.1496, 0.1030, 0.1191, 0.1623, 0.1348, 0.1518,\n",
              "                    0.1371, 0.1317, 0.1020, 0.1424, 0.0978, 0.1328, 0.1562, 0.1492, 0.0747,\n",
              "                    0.0841, 0.1195, 0.0765, 0.1180, 0.1123, 0.1523, 0.2232, 0.1036, 0.0923,\n",
              "                    0.1242, 0.1522, 0.0860, 0.0988, 0.0926, 0.1446, 0.1064, 0.1040, 0.1565,\n",
              "                    0.1157, 0.1476, 0.0973, 0.0988, 0.1578, 0.1193, 0.0973, 0.1160, 0.0872,\n",
              "                    0.0935, 0.0683, 0.1151, 0.0966, 0.0949, 0.1045, 0.1126, 0.1223, 0.1301,\n",
              "                    0.1160, 0.0894, 0.1252, 0.0722, 0.1046, 0.0530, 0.1204, 0.1040, 0.1399,\n",
              "                    0.1081, 0.0634, 0.1074, 0.0844, 0.1086, 0.1038, 0.1231, 0.1161, 0.0774,\n",
              "                    0.1779, 0.1417, 0.0880, 0.0667, 0.0838, 0.1133, 0.1580, 0.1339, 0.1075,\n",
              "                    0.0949, 0.1625, 0.1056, 0.0632, 0.1180, 0.0901, 0.0969, 0.1782, 0.1533,\n",
              "                    0.1038, 0.1441, 0.0792, 0.1248, 0.1075, 0.1050, 0.0793, 0.1119, 0.1319,\n",
              "                    0.0989, 0.0828, 0.0882, 0.1041, 0.1167, 0.2254, 0.0737, 0.1237, 0.2227,\n",
              "                    0.0737, 0.0939, 0.0639, 0.1315, 0.0880, 0.1183, 0.0612, 0.1084, 0.0990,\n",
              "                    0.0704, 0.1531, 0.1251, 0.0875, 0.0811, 0.2117, 0.2200, 0.1231, 0.0572,\n",
              "                    0.1347, 0.1272, 0.1909, 0.2012, 0.1250, 0.0977, 0.1761, 0.1399, 0.0709,\n",
              "                    0.0876, 0.1485, 0.1126, 0.0600, 0.1820, 0.1436, 0.1007, 0.1182, 0.1571,\n",
              "                    0.0831, 0.1483, 0.0903, 0.1074, 0.1506, 0.0849, 0.0983, 0.1100, 0.0787,\n",
              "                    0.1639, 0.1112, 0.0759, 0.0715, 0.0883, 0.0872, 0.0553, 0.0669, 0.1398,\n",
              "                    0.0652, 0.0671, 0.1245, 0.1509, 0.0893, 0.1352, 0.1323, 0.1389, 0.1516,\n",
              "                    0.0854, 0.0796, 0.1359, 0.1097, 0.1042, 0.0681, 0.1141, 0.0787, 0.0997,\n",
              "                    0.1549, 0.1151, 0.1720, 0.1963, 0.0971, 0.1107, 0.0610, 0.1243, 0.1290,\n",
              "                    0.0973, 0.0951, 0.3123, 0.1136, 0.1145, 0.1029, 0.0698, 0.0818, 0.1081,\n",
              "                    0.0872, 0.1230, 0.1005, 0.0926, 0.1073, 0.0963, 0.1453, 0.0735, 0.0995,\n",
              "                    0.1252, 0.1582, 0.1192, 0.0834, 0.1745, 0.1619, 0.0790, 0.2096])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn1): Identity()\n",
              "        (relu1): Identity()\n",
              "        (conv2): ConvBn2d(\n",
              "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
              "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "            min_val=tensor([-4.1776e-02, -1.8305e-02, -2.2439e-04, -1.8278e-02, -1.3787e-02,\n",
              "                    -2.7605e-02, -2.0365e-02, -2.3701e-02, -9.8940e-04, -3.9481e-02,\n",
              "                    -3.0746e-02, -1.6706e-02, -1.5920e-02, -6.7640e-04, -2.4301e-02,\n",
              "                    -2.2683e-03, -2.5325e-02, -2.5345e-02, -2.5715e-03, -2.0258e-02,\n",
              "                    -2.8575e-02, -3.5658e-02, -2.6694e-02, -4.3930e-02, -1.9308e-02,\n",
              "                    -1.6141e-03, -2.3109e-02, -4.4765e-04, -2.3057e-02, -1.9015e-02,\n",
              "                    -2.1664e-02, -2.1952e-02, -5.1197e-03, -2.3050e-02, -1.5124e-03,\n",
              "                    -2.2485e-02, -2.3416e-02, -2.5023e-02, -1.1357e-02, -3.0438e-02,\n",
              "                    -3.1481e-02, -2.0442e-02, -3.0237e-02, -2.2809e-02, -9.9183e-03,\n",
              "                    -9.4290e-03, -2.0940e-02, -1.4832e-03, -1.9800e-02, -3.4412e-02,\n",
              "                    -1.0053e-02, -2.1367e-02, -1.0413e-04, -2.4075e-02, -2.7175e-02,\n",
              "                    -1.8627e-02, -3.7090e-02, -2.4906e-03, -3.3779e-02, -2.8176e-02,\n",
              "                    -2.7965e-04, -2.8157e-02, -2.9627e-02, -2.4915e-02, -2.7078e-02,\n",
              "                    -1.9379e-02, -3.5937e-04, -2.3367e-02, -3.0968e-02, -2.9074e-02,\n",
              "                    -2.6604e-02, -2.2972e-02, -1.2696e-02, -2.0432e-02, -2.1552e-02,\n",
              "                    -2.3228e-02, -3.4767e-02, -2.3270e-02, -2.7045e-02, -2.9532e-02,\n",
              "                    -2.1924e-02, -8.8061e-04, -1.6930e-02, -4.7075e-02, -1.7878e-02,\n",
              "                    -2.8275e-02, -2.5401e-02, -2.2537e-04, -2.7939e-02, -1.9039e-02,\n",
              "                    -5.0463e-04, -2.6810e-02, -1.9102e-02, -2.9077e-02, -2.6895e-02,\n",
              "                    -2.6721e-02, -2.1221e-02, -2.3627e-02, -4.4188e-02, -1.7817e-02,\n",
              "                    -3.1057e-02, -3.1537e-03, -5.2145e-02, -1.9589e-02, -3.1102e-02,\n",
              "                    -2.3849e-02, -2.7421e-02, -3.3883e-02, -2.4895e-02, -9.0832e-03,\n",
              "                    -2.1572e-02, -2.2681e-02, -2.9265e-02, -2.8223e-02, -3.0295e-02,\n",
              "                    -1.1366e-04, -2.6432e-02, -1.2806e-02, -3.3675e-02, -2.5990e-02,\n",
              "                    -1.8890e-02, -1.8139e-02, -3.0745e-02, -2.7947e-02, -1.9143e-02,\n",
              "                    -2.7587e-02, -1.4262e-02, -4.4342e-04, -3.0135e-02, -3.2009e-02,\n",
              "                    -2.0094e-02, -3.8719e-02, -2.9999e-02, -2.6114e-02, -5.7690e-04,\n",
              "                    -3.5529e-03, -2.7740e-02, -6.5969e-04, -1.3528e-02, -2.6893e-02,\n",
              "                    -1.3222e-02, -3.4710e-02, -1.5820e-03, -2.0644e-02, -2.4089e-02,\n",
              "                    -1.6474e-02, -3.7872e-03, -1.4437e-02, -4.8560e-04, -2.9381e-02,\n",
              "                    -2.3192e-02, -4.3571e-02, -8.8739e-04, -1.4825e-02, -1.9193e-02,\n",
              "                    -1.6109e-02, -2.4993e-02, -2.0008e-02, -3.3498e-02, -2.0616e-02,\n",
              "                    -2.9418e-02, -3.7050e-02, -1.7056e-02, -1.4051e-02, -1.9461e-02,\n",
              "                    -1.9690e-02, -2.2547e-02, -2.9087e-02, -2.3991e-02, -2.1902e-02,\n",
              "                    -3.8252e-02, -2.1381e-02, -3.0297e-02, -1.6530e-02, -2.5405e-02,\n",
              "                    -2.1930e-02, -2.7854e-02, -3.4369e-02, -2.5014e-02, -2.5524e-04,\n",
              "                    -2.1931e-02, -1.8116e-02, -2.9758e-02, -1.8772e-02, -2.5994e-02,\n",
              "                    -1.4964e-02, -2.6852e-02, -2.8127e-02, -3.2375e-02, -2.0003e-02,\n",
              "                    -2.5073e-02, -3.5375e-02, -1.6847e-02, -3.0826e-02, -1.7411e-02,\n",
              "                    -2.7036e-02, -2.2072e-02, -9.3922e-03, -3.0327e-02, -1.5796e-02,\n",
              "                    -2.0403e-02, -2.0918e-02, -1.4546e-02, -3.4653e-02, -1.7640e-02,\n",
              "                    -3.4778e-02, -3.3415e-02, -1.9927e-02, -4.3955e-02, -5.9244e-03,\n",
              "                    -2.2469e-02, -1.5972e-02, -2.8394e-02, -3.6290e-04, -2.6678e-02,\n",
              "                    -4.0615e-02, -1.6224e-02, -7.4797e-03, -1.6887e-02, -1.4622e-02,\n",
              "                    -2.7732e-02, -2.4776e-02, -2.5189e-02, -2.9161e-02, -2.6142e-02,\n",
              "                    -1.2040e-02, -2.3047e-02, -1.4768e-02, -3.7496e-02, -1.9905e-02,\n",
              "                    -3.2501e-02, -2.6349e-02, -2.7060e-02, -2.0658e-02, -2.1662e-02,\n",
              "                    -3.1497e-02, -1.8914e-02, -1.7935e-02, -1.6371e-02, -1.4808e-02,\n",
              "                    -2.8585e-02, -1.9203e-02, -1.7623e-02, -3.5463e-02, -2.0598e-02,\n",
              "                    -2.4164e-02, -4.5219e-03, -3.2478e-02, -1.6353e-02, -2.1255e-02,\n",
              "                    -3.2320e-02, -7.3588e-05, -2.1477e-02, -1.3315e-02, -2.8308e-02,\n",
              "                    -2.3255e-02, -1.7390e-02, -2.7114e-02, -2.1069e-02, -2.5694e-02,\n",
              "                    -1.5009e-02, -3.1625e-02, -1.4968e-02, -2.6436e-02, -2.3999e-02,\n",
              "                    -2.3775e-02, -1.6017e-02, -1.7555e-02, -1.9681e-02, -1.2450e-02,\n",
              "                    -1.5747e-02, -2.5566e-02, -2.6253e-02, -1.6115e-02, -1.7660e-02,\n",
              "                    -2.3541e-02, -3.4555e-02, -3.6102e-04, -8.1584e-04, -2.4605e-02,\n",
              "                    -1.9121e-02, -1.6468e-02, -2.4603e-02, -2.1759e-02, -3.1863e-02,\n",
              "                    -2.4969e-02, -2.4388e-03, -2.7342e-02, -2.8850e-02, -3.2978e-02,\n",
              "                    -2.0387e-02, -5.1975e-05, -2.2081e-02, -1.4059e-02, -8.5908e-04,\n",
              "                    -2.4757e-02, -2.9226e-02, -2.5213e-02, -1.8008e-02, -2.2150e-02,\n",
              "                    -1.2628e-02, -2.5863e-02, -2.4144e-02, -1.8195e-02, -3.7588e-04,\n",
              "                    -1.0656e-02, -1.7268e-02, -2.1043e-02, -2.2851e-02, -1.1899e-02,\n",
              "                    -2.1832e-04, -8.4727e-04, -3.1006e-02, -2.7165e-02, -2.2654e-02,\n",
              "                    -2.7172e-02, -1.4630e-02, -2.1858e-02, -1.3946e-02, -1.9128e-04,\n",
              "                    -1.7900e-02, -2.5494e-02, -1.4017e-02, -6.9660e-04, -2.7787e-02,\n",
              "                    -1.8622e-02, -2.8296e-02, -3.9624e-02, -2.6622e-02, -3.6457e-02,\n",
              "                    -1.8087e-02, -3.0310e-02, -1.9529e-02, -2.4150e-02, -3.5740e-02,\n",
              "                    -1.8946e-02, -3.5669e-03, -5.8217e-03, -2.6429e-02, -2.4793e-02,\n",
              "                    -1.7731e-03, -1.7631e-02, -2.3727e-02, -3.8256e-02, -2.2351e-02,\n",
              "                    -2.0510e-02, -3.1117e-02, -3.3891e-02, -2.5119e-02, -1.5393e-02,\n",
              "                    -3.6597e-04, -1.9169e-02, -2.6021e-02, -2.8782e-02, -2.5952e-02,\n",
              "                    -1.3419e-02, -8.6897e-03, -1.6619e-02, -1.4730e-02, -3.1472e-02,\n",
              "                    -1.4776e-02, -2.7752e-02, -1.1879e-02, -2.7962e-02, -1.9797e-04,\n",
              "                    -3.5897e-02, -3.3887e-02, -2.4555e-03, -2.5771e-02, -3.9849e-04,\n",
              "                    -2.8255e-02, -2.9188e-02, -2.9548e-02, -3.4449e-02, -3.2863e-02,\n",
              "                    -1.8688e-02, -3.1334e-02, -1.7977e-02, -2.3965e-02, -2.3268e-02,\n",
              "                    -3.2228e-02, -2.0809e-02, -1.8216e-02, -3.2210e-02, -3.1738e-02,\n",
              "                    -2.0640e-02, -3.7545e-02, -1.6150e-02, -2.3606e-02, -1.3129e-02,\n",
              "                    -1.2818e-02, -2.0229e-02, -1.2069e-02, -2.4797e-02, -2.9538e-02,\n",
              "                    -2.4193e-02, -2.5478e-02, -9.0792e-05, -2.1560e-02, -1.3497e-02,\n",
              "                    -3.6625e-02, -2.9531e-02, -1.4724e-02, -5.3759e-02, -1.7847e-04,\n",
              "                    -2.0161e-02, -3.1878e-02, -2.8610e-04, -2.2903e-02, -1.2099e-02,\n",
              "                    -1.6645e-02, -2.4713e-02, -2.6531e-02, -2.2186e-02, -3.5501e-02,\n",
              "                    -2.1243e-02, -2.4904e-02, -9.0240e-03, -1.4589e-02, -2.0011e-02,\n",
              "                    -1.2385e-02, -2.7610e-02, -2.6989e-02, -3.7408e-02, -3.2931e-02,\n",
              "                    -3.7330e-02, -2.0992e-02, -1.9449e-02, -2.8314e-02, -2.6472e-02,\n",
              "                    -2.7008e-02, -3.2447e-02, -3.4797e-02, -3.6915e-02, -2.8532e-02,\n",
              "                    -2.5162e-02, -3.3616e-02, -2.5915e-02, -2.7605e-04, -2.1121e-02,\n",
              "                    -1.9155e-02, -2.6095e-02, -2.3658e-02, -2.8704e-02, -3.0446e-02,\n",
              "                    -4.3924e-02, -3.0820e-02, -5.3501e-04, -1.4202e-02, -3.2784e-02,\n",
              "                    -2.9826e-02, -4.7278e-02, -2.7102e-02, -3.4134e-02, -3.3262e-02,\n",
              "                    -3.1685e-02, -1.8528e-02, -1.0108e-03, -2.5616e-02, -2.6789e-02,\n",
              "                    -2.6315e-02, -1.4226e-02, -2.7401e-02, -2.5069e-04, -2.5773e-02,\n",
              "                    -1.9039e-02, -2.2104e-02, -3.3221e-02, -2.2055e-02, -1.7673e-02,\n",
              "                    -2.1073e-02, -7.0111e-04, -2.7844e-02, -2.2437e-02, -6.2224e-04,\n",
              "                    -3.2990e-02, -2.4182e-02, -2.7635e-02, -3.1867e-02, -1.7286e-02,\n",
              "                    -2.2218e-02, -5.0947e-02, -3.2621e-02, -2.5901e-02, -2.7779e-02,\n",
              "                    -2.7621e-02, -3.3368e-02, -2.4948e-02, -2.5743e-02, -1.5224e-03,\n",
              "                    -3.2045e-02, -2.2857e-02, -1.2836e-02, -1.8284e-02, -2.5559e-02,\n",
              "                    -2.7781e-02, -5.2554e-04, -1.5285e-02, -4.3367e-03, -3.6759e-02,\n",
              "                    -3.5234e-04, -2.8193e-02, -1.6057e-02, -4.5904e-05, -4.4094e-02,\n",
              "                    -2.5065e-02, -3.3176e-02, -3.8454e-02, -2.4300e-03, -9.2463e-03,\n",
              "                    -3.1157e-02, -2.9828e-02]), max_val=tensor([2.8886e-02, 1.8466e-02, 2.5876e-04, 1.5913e-02, 1.2229e-02, 2.3721e-02,\n",
              "                    1.5054e-02, 2.4211e-02, 7.0837e-04, 4.8009e-02, 2.8307e-02, 1.6844e-02,\n",
              "                    1.2674e-02, 1.0184e-03, 2.0996e-02, 2.2474e-03, 3.4218e-02, 2.2799e-02,\n",
              "                    2.7867e-03, 1.9319e-02, 2.4444e-02, 3.0984e-02, 2.4032e-02, 4.0195e-02,\n",
              "                    1.3420e-02, 1.2263e-03, 2.7486e-02, 5.7844e-04, 1.6308e-02, 1.1572e-02,\n",
              "                    1.8129e-02, 1.5169e-02, 6.7388e-03, 1.6817e-02, 1.7683e-03, 1.3393e-02,\n",
              "                    2.3582e-02, 1.7799e-02, 9.6257e-03, 2.5419e-02, 3.0758e-02, 3.2406e-02,\n",
              "                    2.9359e-02, 1.9661e-02, 1.0355e-02, 5.4091e-03, 1.4595e-02, 1.6793e-03,\n",
              "                    1.6680e-02, 3.0191e-02, 2.3144e-02, 2.0255e-02, 1.1172e-04, 2.6006e-02,\n",
              "                    1.9928e-02, 9.3310e-03, 3.0922e-02, 4.6073e-03, 3.2947e-02, 2.4956e-02,\n",
              "                    3.2008e-04, 2.7311e-02, 2.7978e-02, 2.1293e-02, 2.6437e-02, 1.7317e-02,\n",
              "                    4.1720e-04, 2.9027e-02, 3.0899e-02, 2.6556e-02, 2.8955e-02, 1.9421e-02,\n",
              "                    1.6244e-02, 1.4783e-02, 2.1204e-02, 3.1352e-02, 3.5738e-02, 1.7484e-02,\n",
              "                    2.5918e-02, 2.2516e-02, 2.5682e-02, 9.8627e-04, 1.3164e-02, 3.5456e-02,\n",
              "                    1.5527e-02, 4.7182e-02, 1.7407e-02, 3.3538e-04, 2.2016e-02, 1.1502e-02,\n",
              "                    5.7925e-04, 2.5713e-02, 1.7099e-02, 2.4507e-02, 2.5754e-02, 1.5656e-02,\n",
              "                    1.5736e-02, 2.0047e-02, 3.7993e-02, 1.7895e-02, 2.8139e-02, 3.6492e-03,\n",
              "                    4.0641e-02, 1.6192e-02, 2.3030e-02, 1.6568e-02, 2.6904e-02, 3.0425e-02,\n",
              "                    3.0851e-02, 1.0908e-02, 1.7895e-02, 2.2270e-02, 2.4940e-02, 3.0064e-02,\n",
              "                    2.2278e-02, 1.2208e-04, 3.4314e-02, 1.1955e-02, 2.2871e-02, 3.0995e-02,\n",
              "                    2.1239e-02, 1.5776e-02, 2.4996e-02, 2.2808e-02, 1.8232e-02, 1.9587e-02,\n",
              "                    2.1917e-02, 5.1170e-04, 2.4347e-02, 2.5360e-02, 1.6086e-02, 2.6766e-02,\n",
              "                    2.0566e-02, 2.8372e-02, 7.1191e-04, 5.0314e-03, 2.5246e-02, 7.4522e-04,\n",
              "                    1.0366e-02, 2.5375e-02, 1.4203e-02, 3.3197e-02, 1.6416e-03, 2.0904e-02,\n",
              "                    2.6393e-02, 1.4054e-02, 6.5817e-03, 1.3499e-02, 7.2825e-04, 3.2902e-02,\n",
              "                    2.2725e-02, 2.9479e-02, 1.2569e-03, 1.6354e-02, 1.9974e-02, 1.2786e-02,\n",
              "                    2.2064e-02, 1.6868e-02, 2.3974e-02, 1.9526e-02, 2.6194e-02, 2.9741e-02,\n",
              "                    1.2917e-02, 1.1184e-02, 2.0330e-02, 2.1944e-02, 2.2181e-02, 2.1628e-02,\n",
              "                    4.6635e-02, 2.0449e-02, 2.8263e-02, 1.9796e-02, 2.9961e-02, 1.6021e-02,\n",
              "                    2.1059e-02, 2.2071e-02, 2.1343e-02, 3.2107e-02, 2.7375e-02, 2.5891e-04,\n",
              "                    1.7067e-02, 1.4041e-02, 2.4499e-02, 1.6198e-02, 2.2336e-02, 7.8074e-03,\n",
              "                    2.3428e-02, 2.5994e-02, 3.4713e-02, 1.9498e-02, 1.8038e-02, 2.7595e-02,\n",
              "                    1.5124e-02, 3.1167e-02, 2.4328e-02, 2.2983e-02, 2.2307e-02, 1.3698e-02,\n",
              "                    2.3701e-02, 1.3808e-02, 2.1071e-02, 1.6874e-02, 9.3399e-03, 2.7787e-02,\n",
              "                    1.6613e-02, 2.9265e-02, 3.7831e-02, 1.4192e-02, 3.4490e-02, 8.1072e-03,\n",
              "                    1.7394e-02, 1.3265e-02, 2.6455e-02, 4.9236e-04, 2.7778e-02, 3.5775e-02,\n",
              "                    1.8955e-02, 1.3452e-02, 1.1022e-02, 9.7652e-03, 3.4908e-02, 2.3053e-02,\n",
              "                    2.3760e-02, 3.3671e-02, 2.3718e-02, 8.5338e-03, 1.6980e-02, 1.5047e-02,\n",
              "                    3.8067e-02, 1.2573e-02, 3.0867e-02, 2.2233e-02, 2.0880e-02, 2.0341e-02,\n",
              "                    1.6619e-02, 2.5050e-02, 2.4054e-02, 1.3507e-02, 1.2329e-02, 1.2882e-02,\n",
              "                    2.1672e-02, 1.7400e-02, 1.4833e-02, 3.5650e-02, 2.7018e-02, 1.9526e-02,\n",
              "                    6.9654e-03, 4.2157e-02, 1.2411e-02, 2.2850e-02, 2.2599e-02, 4.0275e-05,\n",
              "                    1.8464e-02, 8.3710e-03, 2.4967e-02, 1.8900e-02, 1.9691e-02, 2.4759e-02,\n",
              "                    1.6839e-02, 1.9195e-02, 1.2307e-02, 2.7755e-02, 1.4149e-02, 2.2778e-02,\n",
              "                    2.6351e-02, 1.4811e-02, 1.5196e-02, 1.1760e-02, 1.6640e-02, 1.1313e-02,\n",
              "                    1.8893e-02, 1.4850e-02, 3.2360e-02, 2.0544e-02, 1.6690e-02, 1.6690e-02,\n",
              "                    3.6261e-02, 7.1006e-04, 1.5159e-03, 1.9116e-02, 1.4953e-02, 1.4358e-02,\n",
              "                    2.3215e-02, 2.2012e-02, 2.6789e-02, 2.3195e-02, 3.0610e-03, 2.2950e-02,\n",
              "                    2.6452e-02, 3.5974e-02, 1.9784e-02, 2.7710e-05, 1.2399e-02, 1.0388e-02,\n",
              "                    5.8606e-04, 2.3014e-02, 2.9255e-02, 2.5619e-02, 1.4435e-02, 2.7714e-02,\n",
              "                    1.9189e-02, 2.1231e-02, 1.9366e-02, 1.3938e-02, 2.3827e-04, 1.0941e-02,\n",
              "                    1.2548e-02, 2.0201e-02, 2.4691e-02, 1.2484e-02, 2.6801e-04, 1.1413e-03,\n",
              "                    2.8324e-02, 2.4887e-02, 2.8049e-02, 2.4673e-02, 1.1512e-02, 2.0573e-02,\n",
              "                    1.9860e-02, 2.0453e-04, 1.2497e-02, 2.9087e-02, 1.3466e-02, 8.6160e-04,\n",
              "                    3.3967e-02, 1.5312e-02, 2.6607e-02, 4.8638e-02, 2.4861e-02, 3.4104e-02,\n",
              "                    1.1841e-02, 2.2549e-02, 1.2489e-02, 2.5079e-02, 3.4075e-02, 1.8586e-02,\n",
              "                    3.9045e-03, 7.3956e-03, 4.4781e-02, 2.0773e-02, 3.3038e-03, 1.1801e-02,\n",
              "                    1.8307e-02, 5.2349e-02, 2.1197e-02, 2.0157e-02, 2.6188e-02, 2.7818e-02,\n",
              "                    2.4223e-02, 1.4185e-02, 5.0755e-04, 1.7448e-02, 2.5923e-02, 1.9021e-02,\n",
              "                    2.4939e-02, 1.0838e-02, 9.5688e-03, 1.2884e-02, 1.2687e-02, 2.4100e-02,\n",
              "                    1.2524e-02, 2.3191e-02, 1.4538e-02, 2.0012e-02, 2.9116e-04, 2.5220e-02,\n",
              "                    3.9896e-02, 3.1981e-03, 2.4300e-02, 4.6712e-04, 2.0602e-02, 3.1290e-02,\n",
              "                    2.0129e-02, 3.3777e-02, 2.2655e-02, 1.4914e-02, 2.5811e-02, 2.0655e-02,\n",
              "                    2.9043e-02, 2.2220e-02, 2.3473e-02, 1.8921e-02, 1.9971e-02, 2.2793e-02,\n",
              "                    2.9833e-02, 2.2258e-02, 3.2348e-02, 9.4154e-03, 2.0291e-02, 1.3659e-02,\n",
              "                    1.8136e-02, 1.9250e-02, 1.5876e-02, 2.2424e-02, 2.4033e-02, 1.8503e-02,\n",
              "                    1.9752e-02, 1.1071e-04, 1.7018e-02, 1.2424e-02, 2.5021e-02, 2.4609e-02,\n",
              "                    1.1780e-02, 4.7727e-02, 2.5276e-04, 1.7702e-02, 3.1265e-02, 4.2782e-04,\n",
              "                    1.6394e-02, 7.0757e-03, 1.1518e-02, 2.2096e-02, 1.7507e-02, 2.2084e-02,\n",
              "                    4.8545e-02, 2.6831e-02, 2.2679e-02, 9.9751e-03, 1.5266e-02, 2.8254e-02,\n",
              "                    1.1804e-02, 2.3207e-02, 3.0636e-02, 3.1614e-02, 2.9179e-02, 3.0996e-02,\n",
              "                    1.9053e-02, 2.1795e-02, 3.0567e-02, 4.1417e-02, 2.1044e-02, 2.3902e-02,\n",
              "                    3.4105e-02, 3.4571e-02, 2.0450e-02, 1.7822e-02, 2.3636e-02, 1.6349e-02,\n",
              "                    3.1144e-04, 2.2966e-02, 1.8737e-02, 2.3233e-02, 1.9572e-02, 1.8439e-02,\n",
              "                    2.5774e-02, 2.3333e-02, 2.8037e-02, 6.5788e-04, 1.0931e-02, 2.5359e-02,\n",
              "                    2.8522e-02, 3.6115e-02, 2.5366e-02, 2.9878e-02, 2.0834e-02, 2.4581e-02,\n",
              "                    1.5278e-02, 1.3184e-03, 2.5185e-02, 2.1265e-02, 3.4062e-02, 1.2656e-02,\n",
              "                    3.6003e-02, 4.5739e-04, 2.1952e-02, 1.3658e-02, 2.6662e-02, 3.2448e-02,\n",
              "                    1.9559e-02, 1.1294e-02, 1.7712e-02, 1.0440e-03, 2.1569e-02, 2.4693e-02,\n",
              "                    9.1455e-04, 2.9503e-02, 2.1955e-02, 2.7183e-02, 3.6558e-02, 1.8028e-02,\n",
              "                    2.4447e-02, 4.6260e-02, 3.1098e-02, 2.4738e-02, 2.1313e-02, 2.3492e-02,\n",
              "                    2.3413e-02, 1.9960e-02, 2.1271e-02, 1.6985e-03, 3.6592e-02, 2.3932e-02,\n",
              "                    1.4546e-02, 1.9843e-02, 2.4822e-02, 1.8184e-02, 5.8776e-04, 1.3950e-02,\n",
              "                    5.7305e-03, 4.7024e-02, 4.8880e-04, 2.2893e-02, 1.3497e-02, 9.1938e-05,\n",
              "                    3.5745e-02, 2.9729e-02, 2.5325e-02, 3.0432e-02, 2.8892e-03, 1.0004e-02,\n",
              "                    4.2347e-02, 1.7696e-02])\n",
              "          )\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (bn2): Identity()\n",
              "        (skip_add): FloatFunctional(\n",
              "          (activation_post_process): HistogramObserver()\n",
              "        )\n",
              "        (relu2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "    (fc): Linear(\n",
              "      in_features=512, out_features=10, bias=True\n",
              "      (weight_fake_quant): PerChannelMinMaxObserver(\n",
              "        min_val=tensor([-0.4583, -0.5694, -0.3281, -0.4043, -0.3889, -0.3866, -0.3823, -0.3545,\n",
              "                -0.5853, -0.3567]), max_val=tensor([0.4111, 0.6390, 0.4366, 0.4035, 0.4592, 0.3437, 0.5078, 0.3717, 0.4871,\n",
              "                0.5218])\n",
              "      )\n",
              "      (activation_post_process): HistogramObserver()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using high-level static quantization wrapper\n",
        "# The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
        "# quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
        "\n",
        "quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
        "\n",
        "quantized_model.eval()\n",
        "\n",
        "# Print quantized model.\n",
        "print(quantized_model)\n",
        "\n",
        "# Save quantized model.\n",
        "save_torchscript_model(model=quantized_model,\n",
        "                           model_dir=model_dir,\n",
        "                           model_filename=quantized_model_filename)\n",
        "\n",
        "# Load quantized model.\n",
        "quantized_jit_model = load_torchscript_model(\n",
        "        model_filepath=quantized_model_filepath, device=cpu_device)\n",
        "\n",
        "_, fp32_eval_accuracy = evaluate_model(model=model,\n",
        "                                           test_loader=test_loader,\n",
        "                                           device=cpu_device,\n",
        "                                           criterion=None)\n",
        "_, int8_eval_accuracy = evaluate_model(model=quantized_jit_model,\n",
        "                                           test_loader=test_loader,\n",
        "                                           device=cpu_device,\n",
        "                                           criterion=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynEGaTl-yeTT",
        "outputId": "87cf476d-2c4a-4837-c25a-82c031dd9cf9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QuantizedResNet18(\n",
            "  (quant): Quantize(scale=tensor([0.0374]), zero_point=tensor([57]), dtype=torch.quint8)\n",
            "  (dequant): DeQuantize()\n",
            "  (model_fp32): ResNet(\n",
            "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.10260126739740372, zero_point=0, padding=(3, 3))\n",
            "    (bn1): Identity()\n",
            "    (relu): Identity()\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03684097155928612, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07889273762702942, zero_point=64, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.1623239666223526, zero_point=34\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.03466548025608063, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.11840449273586273, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.14951542019844055, zero_point=36\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.059645965695381165, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.18283802270889282, zero_point=53, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.10774164646863937, zero_point=58)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.16588498651981354, zero_point=64\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.14526888728141785, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.5087238550186157, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.5215334892272949, zero_point=54\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.3044006824493408, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.7082080841064453, zero_point=47, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.3371335566043854, zero_point=69)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.784136176109314, zero_point=54\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.4834028482437134, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.0625401735305786, zero_point=60, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=1.2810763120651245, zero_point=46\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.5738968849182129, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.0299062728881836, zero_point=65, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (downsample): Sequential(\n",
            "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=1.0078366994857788, zero_point=67)\n",
            "          (1): Identity()\n",
            "        )\n",
            "        (skip_add): QFunctional(\n",
            "          scale=1.0623736381530762, zero_point=99\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.8268343210220337, zero_point=0, padding=(1, 1))\n",
            "        (bn1): Identity()\n",
            "        (relu1): Identity()\n",
            "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.8988916873931885, zero_point=124, padding=(1, 1))\n",
            "        (bn2): Identity()\n",
            "        (skip_add): QFunctional(\n",
            "          scale=0.8330718278884888, zero_point=123\n",
            "          (activation_post_process): Identity()\n",
            "        )\n",
            "        (relu2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.2765270471572876, zero_point=65, qscheme=torch.per_channel_affine)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this assertion since the values might deviate a lot.\n",
        "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
        "\n",
        "print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
        "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
        "\n",
        "fp32_cpu_inference_latency = measure_inference_latency(model=model,\n",
        "                                                           device=cpu_device,\n",
        "                                                           input_size=(1, 3,\n",
        "                                                                       32, 32),\n",
        "                                                           num_samples=100)\n",
        "int8_cpu_inference_latency = measure_inference_latency(\n",
        "        model=quantized_model,\n",
        "        device=cpu_device,\n",
        "        input_size=(1, 3, 32, 32),\n",
        "        num_samples=100)\n",
        "int8_jit_cpu_inference_latency = measure_inference_latency(\n",
        "        model=quantized_jit_model,\n",
        "        device=cpu_device,\n",
        "        input_size=(1, 3, 32, 32),\n",
        "        num_samples=100)\n",
        "fp32_gpu_inference_latency = measure_inference_latency(model=model,\n",
        "                                                           device=cuda_device,\n",
        "                                                           input_size=(1, 3,\n",
        "                                                                       32, 32),\n",
        "                                                           num_samples=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju6e0diMybAj",
        "outputId": "dd66aaed-f326-47b1-c3e0-2bcff91798a4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 evaluation accuracy: 0.649\n",
            "INT8 evaluation accuracy: 0.675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(\n",
        "        fp32_cpu_inference_latency * 1000))\n",
        "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(\n",
        "        fp32_gpu_inference_latency * 1000))\n",
        "print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(\n",
        "        int8_cpu_inference_latency * 1000))\n",
        "print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(\n",
        "        int8_jit_cpu_inference_latency * 1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fSfV6NKyZjX",
        "outputId": "91c5b56d-1755-4547-c960-a4bbd25652f8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 CPU Inference Latency: 8.36 ms / sample\n",
            "FP32 CUDA Inference Latency: 5.95 ms / sample\n",
            "INT8 CPU Inference Latency: 5.71 ms / sample\n",
            "INT8 JIT CPU Inference Latency: 1.66 ms / sample\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4Su3x5d1rmG9"
      },
      "execution_count": 86,
      "outputs": []
    }
  ]
}